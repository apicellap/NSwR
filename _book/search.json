[{"path":"index.html","id":"about","chapter":"About","heading":"About","text":"resource encompasses compiled notes Dr. Andy Hector’s text, New Statistics R: Introduction Biologists Second edition. Throughout book emphasizes use Rmarkdown reproducible research. take recommendation step taking R-related notes Rmarkdown documents. Nearly every section header corresponds Dr. Hector’s chapter headings. notes meant replace textbook. Instead may act helpful guide written someone student biostatistics.","code":"\nsessionInfo()\n#> R version 4.1.1 (2021-08-10)\n#> Platform: aarch64-apple-darwin20 (64-bit)\n#> Running under: macOS Monterey 12.4\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.0.dylib\n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib\n#> \n#> locale:\n#> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets \n#> [6] methods   base     \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] bookdown_0.27   digest_0.6.29   R6_2.5.1       \n#>  [4] jsonlite_1.7.3  magrittr_2.0.3  evaluate_0.15  \n#>  [7] stringi_1.7.6   cachem_1.0.6    rlang_1.0.2    \n#> [10] cli_3.2.0       fs_1.5.2        rstudioapi_0.13\n#> [13] jquerylib_0.1.4 xml2_1.3.3      bslib_0.3.1    \n#> [16] rmarkdown_2.14  tools_4.1.1     stringr_1.4.0  \n#> [19] xfun_0.31       yaml_2.2.1      fastmap_1.1.0  \n#> [22] compiler_4.1.1  memoise_2.0.1   htmltools_0.5.2\n#> [25] downlit_0.4.0   knitr_1.39      sass_0.4.0"},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"introduction.html","id":"intro","chapter":"1 Introduction","heading":"1.1 Intro","text":"","code":""},{"path":"introduction.html","id":"aim-of-this-book","chapter":"1 Introduction","heading":"1.2 Aim of this book","text":"intended use text introduce useful types statistical analysis including linear models generalized linear model (GLM) extensionsThe approach learn using real datasets relating biological environmental sciences","code":""},{"path":"introduction.html","id":"changes-in-the-second-edition","chapter":"1 Introduction","heading":"1.3 Changes in the second edition","text":"","code":""},{"path":"introduction.html","id":"the-r-programming-language-for-statistics-and-graphics","chapter":"1 Introduction","heading":"1.4 The R programming language for statistics and graphics","text":"","code":""},{"path":"introduction.html","id":"scope","chapter":"1 Introduction","heading":"1.5 Scope","text":"focus linear model framework since applies biological sciences wellGLMs introduced non-normal distributions","code":""},{"path":"introduction.html","id":"what-is-not-covered","chapter":"1 Introduction","heading":"1.6 What is not covered","text":"Non-linear regression approaches, generalized additive models, non-parametric approaches","code":""},{"path":"introduction.html","id":"the-approach","chapter":"1 Introduction","heading":"1.7 The approach","text":"methods text belong ‘classist frequentist statistics’approach come scrutiny due reliance probability (p) values lowered emphasis effect sizes (estimates intervals)author also pretty vocal criticism uses estimation-based approaches focuses estimates confidence intervals possibleHe also uses priori contrasts (comparisons planned advance) encourages avoidiing inappropriate overuse multiple testing instead implement thoughtful/planned approach","code":""},{"path":"introduction.html","id":"the-new-statistics","chapter":"1 Introduction","heading":"1.8 The new statistics?","text":"term basically refers criticisms mentioned last section recent focus use confidence intervals estimatesThis combined use modern maximum likelihood-based analysis methods reproducible research","code":""},{"path":"introduction.html","id":"getting-started","chapter":"1 Introduction","heading":"1.9 Getting started","text":"introduction R section end textThe author dives right things next chapter","code":""},{"path":"motivation.html","id":"motivation","chapter":"2 Motivation","heading":"2 Motivation","text":"","code":""},{"path":"motivation.html","id":"a-matter-of-life-and-death","chapter":"2 Motivation","heading":"2.1 A matter of life and death","text":"author introduces chapter anecdote Space Shuttle Challenger built 1986It infamous exploding soonafter launch, killing astronauts aboardAn investigation disaster found o-rings, otherwise prevented fuel leaks, failedThe investigation found failure result colder typical temperatures day launch, causing o-rings become brittle allow fuel leakData investigation can found package ‘faraway’Plot data orings dataset:question data pose number leaks associated temperature. NASA scientists engineers draw graph like ask question.","code":"\ninstall.packages(\"faraway\",repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",repos = \"https://cran.us.r-project.org\")\nlibrary(faraway)\nlibrary(ggplot2)\n\nhead(orings)\nfig2_2 <- ggplot(orings, aes(x = temp, y = damage)) + geom_point() + theme_bw()\nfig2_2"},{"path":"motivation.html","id":"summary-statistics","chapter":"2 Motivation","heading":"2.2 Summary: statistics","text":"Challenger o-rings data complex now, author start data Charles Darwin","code":""},{"path":"motivation.html","id":"summary-r","chapter":"2 Motivation","heading":"2.3 Summary: R","text":"","code":""},{"path":"description.html","id":"description","chapter":"3 Description","heading":"3 Description","text":"","code":""},{"path":"description.html","id":"introduction-1","chapter":"3 Description","heading":"3.1 Introduction","text":"Author starts chapter introducing linear-model analysis goal compare effects different treatments\ntreatment applied group experimental units (biological replicates)\ntreatments produce change replicates, quantifiable difference groups\ntreatment applied group experimental units (biological replicates)treatments produce change replicates, quantifiable difference groupsThis chapter focuses treatment effects hypohthesis testing saved later","code":""},{"path":"description.html","id":"darwins-maize-pollination-data","chapter":"3 Description","heading":"3.2 Darwin’s maize pollination data","text":"Darwin’s book Effects Cross Self-Fertilization Vegetable Kingdom (1876) describes produced maize seeds pollinating flowers parent individual pollen another plantPairs seeds taken selfed -crossed plants grown potsThe height young seedlings recorded measure fitnessDarwin wanted know whether inbreeding reduced fitness selfed plantsDarwin’s data package ‘SMPracticals’dataframe contains data ‘darwin’R terminology\nsingle values = scalars\ncolumns numerical data = vectors\nsingle values = scalarscolumns numerical data = vectors","code":"\nhead(darwin)\n#>   pot pair  type height\n#> 1   I    1 Cross 23.500\n#> 2   I    1  Self 17.375\n#> 3   I    2 Cross 12.000\n#> 4   I    2  Self 20.375\n#> 5   I    3 Cross 21.000\n#> 6   I    3  Self 20.000"},{"path":"description.html","id":"known-your-data","chapter":"3 Description","heading":"3.2.1 Known your data","text":"Understand structure dataframe:Structure:\nfirst three variables Factors\ncategorical varaibles divide data discrete groups called levels\n\nlast variable numerical one. continuous measure heigth\nfirst three variables Factors\ncategorical varaibles divide data discrete groups called levels\ncategorical varaibles divide data discrete groups called levelsThe last variable numerical one. continuous measure heigthGenerate summary statistics dataframe:","code":"\nstr(darwin)\n#> 'data.frame':    30 obs. of  4 variables:\n#>  $ pot   : Factor w/ 4 levels \"I\",\"II\",\"III\",..: 1 1 1 1 1 1 2 2 2 2 ...\n#>  $ pair  : Factor w/ 15 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 2 2 3 3 4 4 5 5 ...\n#>  $ type  : Factor w/ 2 levels \"Cross\",\"Self\": 1 2 1 2 1 2 1 2 1 2 ...\n#>  $ height: num  23.5 17.4 12 20.4 21 ...\nsummary(darwin)\n#>   pot          pair       type        height     \n#>  I  : 6   1      : 2   Cross:15   Min.   :12.00  \n#>  II : 6   2      : 2   Self :15   1st Qu.:17.53  \n#>  III:10   3      : 2              Median :18.88  \n#>  IV : 8   4      : 2              Mean   :18.88  \n#>           5      : 2              3rd Qu.:21.38  \n#>           6      : 2              Max.   :23.50  \n#>           (Other):18"},{"path":"description.html","id":"summarizing-and-describing-data","chapter":"3 Description","heading":"3.2.2 Summarizing and describing data","text":"Plant height response variableThe treatments involved two types hand-pollination (selfing crossing). explanatory variableThe goal see can explain differences plant height function pollination treatmentThere 15 replicates treatment group, planted pairsThis dataframe long tidy format, typically idealDarwin data wide format can found package, Sleuth3, specifcally dataframe, exo0428Visualize data boxplot:Figure takeaways:\nBold central horizontal line median\nTop horizontal part box third quartile\nBottom horizontal part box first quartile\ncontain middle data\n\nwhiskers contain 95% data\ndatapoints outliers\nBold central horizontal line medianTop horizontal part box third quartileBottom horizontal part box first quartile\ncontain middle data\ncontain middle dataThe whiskers contain 95% dataThe datapoints outliersViolin plot can show higher lower ‘centers gravity’ distributions data:author sums goal analysis quoting statistician, Nate Silver, says aim find systematic pattern (signal) data stands background variability (noise)\nSignal often quantified finding average (also median mode)\nSignal often quantified finding average (also median mode)Determine mean data (ignoring treatment groups):Now problem quantify variability.Variance quantified process called ‘least squares’, laterVariance also known mean squares referred \n\\(S^{2}\\) sample data discussed () \n\\(\\sigma\\)\\(^{2}\\) (sigma) population sample drawn discussed\n\\(S^{2}\\) sample data discussed () \\(\\sigma\\)\\(^{2}\\) (sigma) population sample drawn discussedCalculate variance data whole:catch estimate variability scale dataThis squared scale, data hand unsquaredThis means measure signal (plant height) inches varibility plant height squared inches, doesn’t make senseSo take square root variance get back scale original data\nEffectively, means find standard devation (SD):\n\n\\(SD = \\sqrt{s^2}\\)\n\nEffectively, means find standard devation (SD):\n\n\\(SD = \\sqrt{s^2}\\)\n, \\(\\sigma\\) used referring whole population standard deviationThe latin \\(s\\) used sample SDSD average difference individual measurement (.e. single plant height measurement) mean valueCalculate SD R:Double check derivation SD:","code":"\nhead(ex0428)\n#>   Cross  Self\n#> 1 23.50 17.38\n#> 2 12.00 20.38\n#> 3 21.00 20.00\n#> 4 22.00 20.00\n#> 5 19.13 18.38\n#> 6 21.50 18.63\nfig3_1 <- ggplot(darwin, aes(x=type, y=height)) + geom_boxplot() + \n  ylab(\"height (in)\") + theme_bw() + scale_y_continuous(limits = c(0,25),  breaks=c(0,5,10,15,20,25))\nfig3_1\nfig3_2 <- ggplot(darwin, aes(x=type, y=height)) +\n  geom_violin() + \n  ylab(\"height (in)\") + theme_bw() + scale_y_continuous(limits = c(0,25),  breaks=c(0,5,10,15,20,25)) + geom_jitter()\nfig3_2\nwith(data = darwin, mean(height)) #average height for all 30 plants \n#> [1] 18.88333\nvar(darwin$height) \n#> [1] 10.11846\na <- sd(darwin$height)\na\n#> [1] 3.180953\nb = sd(darwin$height)\nisTRUE(a==b)\n#> [1] TRUE"},{"path":"description.html","id":"comparing-groups","chapter":"3 Description","heading":"3.2.3 Comparing groups","text":"Calculate means groups (approach scale well):Calculate summary statistics efficiently:Assign calculated values objects:Create new column contain text soon merge dataframe:Plot summary statistics:","code":"\nmean(ex0428$Cross)\n#> [1] 20.19333\nmean(ex0428$Self)\n#> [1] 17.57667\nwith(data = darwin, tapply(height, type, mean))\n#>    Cross     Self \n#> 20.19167 17.57500\nwith(data = darwin, tapply(height, type, sd))\n#>    Cross     Self \n#> 3.616945 2.051676\nmeans <- tapply(darwin$height, darwin$type,mean)\nsds <- tapply(darwin$height, darwin$type,sd)\npollination <- c(\"Crossed\",\"Selfed\")\ndar_sum_stats <- data.frame(pollination, means, sds) \ndar_sum_stats\n#>       pollination    means      sds\n#> Cross     Crossed 20.19167 3.616945\n#> Self       Selfed 17.57500 2.051676\nfig3_3 <- ggplot(dar_sum_stats, aes(x=pollination, y=means)) +\n  geom_pointrange(aes(ymin = means - sds, max = means + sds)) + \n  ylab(\"height (in)\") + theme_bw() \nfig3_3"},{"path":"reproducible-research.html","id":"reproducible-research","chapter":"4 Reproducible Research","heading":"4 Reproducible Research","text":"","code":""},{"path":"reproducible-research.html","id":"the-reproducibility-crisis","chapter":"4 Reproducible Research","heading":"4.1 The reproducibility crisis","text":"concerning amount research studies discovered last decade reproducibleOne aim chapter demonstrate write script convert Rmarkdown document","code":""},{"path":"reproducible-research.html","id":"r-scripts","chapter":"4 Reproducible Research","heading":"4.2 R Scripts","text":"Summary statistics dataframe atmospheric carbon dioxide concentrations:Plot data:","code":"\nsummary(co2)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   313.2   323.5   335.2   337.1   350.3   366.8\nplot(co2)"},{"path":"reproducible-research.html","id":"analysis-notebooks","chapter":"4 Reproducible Research","heading":"4.3 Analysis notebooks","text":"author says one way combat reproducibility crisis document exactly data analysis doneThis can accomplished RMarkdownScripts can still written .R files, converted .Rmd files ","code":""},{"path":"reproducible-research.html","id":"rmarkdown","chapter":"4 Reproducible Research","heading":"4.4 RMarkdown","text":"gives basic overview RMarkdown including convert pdfHe emphasizes R updated roughly every six monthsRecord version R used create script running:","code":"\nsessionInfo()\n#> R version 4.1.1 (2021-08-10)\n#> Platform: aarch64-apple-darwin20 (64-bit)\n#> Running under: macOS Monterey 12.4\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.0.dylib\n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib\n#> \n#> locale:\n#> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets \n#> [6] methods   base     \n#> \n#> other attached packages:\n#> [1] SMPracticals_1.4-3 ellipse_0.4.3      ggplot2_3.3.6     \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] tidyselect_1.1.2 xfun_0.31        bslib_0.3.1     \n#>  [4] purrr_0.3.4      splines_4.1.1    lattice_0.20-44 \n#>  [7] colorspace_2.0-3 vctrs_0.4.1      generics_0.1.2  \n#> [10] htmltools_0.5.2  yaml_2.2.1       utf8_1.2.2      \n#> [13] survival_3.2-11  rlang_1.0.2      jquerylib_0.1.4 \n#> [16] pillar_1.7.0     glue_1.6.1       withr_2.4.3     \n#> [19] DBI_1.1.2        lifecycle_1.0.1  stringr_1.4.0   \n#> [22] munsell_0.5.0    gtable_0.3.0     memoise_2.0.1   \n#> [25] evaluate_0.15    knitr_1.39       fastmap_1.1.0   \n#> [28] fansi_1.0.2      highr_0.9        scales_1.1.1    \n#> [31] cachem_1.0.6     jsonlite_1.7.3   fs_1.5.2        \n#> [34] digest_0.6.29    stringi_1.7.6    bookdown_0.27   \n#> [37] dplyr_1.0.9      grid_4.1.1       cli_3.2.0       \n#> [40] tools_4.1.1      magrittr_2.0.3   sass_0.4.0      \n#> [43] tibble_3.1.6     crayon_1.5.0     pkgconfig_2.0.3 \n#> [46] downlit_0.4.0    Matrix_1.3-4     ellipsis_0.3.2  \n#> [49] MASS_7.3-58      xml2_1.3.3       assertthat_0.2.1\n#> [52] rmarkdown_2.14   rstudioapi_0.13  R6_2.5.1        \n#> [55] nlme_3.1-152     compiler_4.1.1"},{"path":"estimation.html","id":"estimation","chapter":"5 Estimation","heading":"5 Estimation","text":"","code":""},{"path":"estimation.html","id":"introduction-2","chapter":"5 Estimation","heading":"5.1 Introduction","text":"Darwin’s pollination experiments, wanted find whether selfing detrimental plant fitness. question concerns differences plant heightThis chapter addresses estimating mean heights two pollination treatments difference + quantify confidence estimates used determine whether statistically different","code":"\ninstall.packages(\"arm\", repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\", repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"Sleuth3\", repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"SMPracticals\", repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(Sleuth3)\nlibrary(SMPracticals)"},{"path":"estimation.html","id":"quick-tests","chapter":"5 Estimation","heading":"5.2 Quick tests","text":"Author talks Tukey quick tests assess variability groups data (treatments)\nmakeshift outliers data can’t actually run test outliers\nignores purpose exercise\nmakeshift outliers data can’t actually run test outliersBut ignores purpose exerciseThe test supposed determine whether relative shift y values one group compared :Plot quick tukey test:Figure takeaways:\noutliers ignored, test show fairly strong upward shift height values cross pollinated progeny relative selfed progeny\nsupport Darwin’s hypothesis fitness outcrossing\noutliers ignored, test show fairly strong upward shift height values cross pollinated progeny relative selfed progenyThis support Darwin’s hypothesis fitness outcrossing","code":"\nfig5_1 <- ggplot(darwin, aes(x = type, y = height)) + \n  geom_point() +\n  geom_hline(yintercept = c(18.1, 20.5), #draw line that intercepts with the y axis at these y values \n             colour = c(\"blue\", \"red\"),  #give these dotted lines these colors, respectively \n              linetype = 2) + #linetype 2 is a dotted line; 1 is solid \n  theme_bw() + \n  scale_y_continuous(limits = c(10,25), minor_breaks = seq(10,25,1),\n                     breaks =seq(10, 25,1)) \n              \nfig5_1    "},{"path":"estimation.html","id":"differences-between-groups","chapter":"5 Estimation","heading":"5.3 Differences between groups","text":"Since Darwin’s data consists matched pairs seedlings, better work differences height rather treatment meansCalculate difference value Cross Self data new column:Calculate mean difference column:Calculate sd difference column:Standard deviation assessing value variation within one sample (population)\n’s average distance datapoint mean\n’s testing confident difference samples exists\n’s average distance datapoint meanIt’s testing confident difference samples existsThat’s standard error confidence intervals come ","code":"\nex0428$difference <- ex0428$Cross - ex0428$Self \nhead(ex0428)\n#>   Cross  Self difference\n#> 1 23.50 17.38       6.12\n#> 2 12.00 20.38      -8.38\n#> 3 21.00 20.00       1.00\n#> 4 22.00 20.00       2.00\n#> 5 19.13 18.38       0.75\n#> 6 21.50 18.63       2.87\nmean(ex0428$difference) \n#> [1] 2.616667\nsd(ex0428$difference)\n#> [1] 4.719373"},{"path":"estimation.html","id":"standard-deviations-and-standard-errors","chapter":"5 Estimation","heading":"5.4 Standard deviations and standard errors","text":", standard deviation descriptive statistic means variability (dispersion) sample data\napplies individual datapoints\napplies individual datapointsShifting focus individual datapoints comparing means, need utilize standard errorStandard error measure variance sample size standard deviation factored :\\(\\sigma\\) used dealing population whole\\(s^2\\) - sample variance (, \\(\\sigma^2\\) used entire population addressed)\\(n\\) = sample size\nStandard error describes variability around mean expected among samples experiment repeated \nnew mean data collected experiments likely fall within SE bounds\n\nSince sample size denominator, higher sample size decreases SE value.\nreflects decreasing level uncertainty data, diminishing returns relationship. halve uncertainty, sample size must quadrupled\nStandard error describes variability around mean expected among samples experiment repeated \nnew mean data collected experiments likely fall within SE bounds\nnew mean data collected experiments likely fall within SE boundsSince sample size denominator, higher sample size decreases SE value.reflects decreasing level uncertainty data, diminishing returns relationship. halve uncertainty, sample size must quadrupledCalculate standard error mean difference height:Assess distribution data:null hypothesis can used understand SEs can used infer level confidence data whether difference groupsNull hypothesis - difference (difference 0) crossed selfed progeny terms mean height\nmean difference 2.62 inches\nGiven mean difference, need determine likely estimated difference (2.62) null hypothesis correct difference treatments. normal distribution used ascertain .\nmean difference 2.62 inchesGiven mean difference, need determine likely estimated difference (2.62) null hypothesis correct difference treatments. normal distribution used ascertain .","code":"\nx<-format(round(sd(ex0428$difference)/sqrt(15), 2), nsmall = 2) #round to 2 decimal places \nx <- as.numeric(x)\nx\n#> [1] 1.22\nggplot(darwin, aes(x = height)) + geom_histogram(bins=25)"},{"path":"estimation.html","id":"the-normal-distribution-and-the-central-limit-theorem","chapter":"5 Estimation","heading":"5.5 The normal distribution and the central limit theorem","text":"Defined two parameters: mean standard deviation (SD)\nBell curve centered mean (value zero x axis)\n67.8% data within 1 SD mean\n95% data within 2 SDs mean\n99.8% data within 3 SDs mean\nBell curve centered mean (value zero x axis)67.8% data within 1 SD mean95% data within 2 SDs mean99.8% data within 3 SDs meanWe use normal distribution model data assume data approximately normal.\nformally test making models looking diagnostics\nformally test making models looking diagnosticsSide note: using properties normal distribution model variability sample works less effectively small sample sizesWe asking, likely see difference 2.62 inches null hypothesis true? many SEs away zero 2.62?assume data approximately normal can use normal distribution model data onFigure 5.3\ncenter bell curve mean (2.62 inches)\nred line, shows 0, beyond two standard errors mean\n0 lies beyond 2 SEs mean, outside 95& data, can reject null hypothesis difference height level confidence\nlowest level confidence statistical significance\nfail reject null hypothesis confidence level 99% (three SEs) greater\ncenter bell curve mean (2.62 inches)red line, shows 0, beyond two standard errors meanBecause 0 lies beyond 2 SEs mean, outside 95& data, can reject null hypothesis difference height level confidenceThis lowest level confidence statistical significanceWe fail reject null hypothesis confidence level 99% (three SEs) greater","code":"\nknitr::include_graphics(\"fig5_2.png\")"},{"path":"estimation.html","id":"confidence-intervals","chapter":"5 Estimation","heading":"5.6 Confidence intervals","text":"Two standard errors encompasesses central 95% data normal curve\nreferred 95% confidence interval (CI)\nCIs defined upper lower limits (bounds)\ncan calculated following ways:\nreferred 95% confidence interval (CI)CIs defined upper lower limits (bounds)can calculated following ways:Calculate lower bound 95% CI:Calculate upper bound 95% CI:Conventionally, 95% CI calculated upper lower bounds 2 SEs meanThe 95% refers imaginary repetition experiments/samples interval capture true value (.e. unknown population mean) 95% time (long term sense)Report mean difference height 2.62 [0.18, 5.06] inches (95% CI)CI takeaways:\nsince 0 outside 95% CI bounds, can reject null hypothesis level\nfail 99% CI (3 standard errors mean)\nsince 0 outside 95% CI bounds, can reject null hypothesis levelbut fail 99% CI (3 standard errors mean)","code":"\n2.62 - (2*x) #x = 1.22. This is one standard error of the mean. \n#> [1] 0.18\n2.62 + (2*x) \n#> [1] 5.06"},{"path":"linear-models.html","id":"linear-models","chapter":"6 Linear models","heading":"6 Linear models","text":"Chapter 5 introduced Darwin’s maize data, manual calculation confidence intervals (CIs)chapter, author discusses efficient methods calculating CIs","code":""},{"path":"linear-models.html","id":"introduction-3","chapter":"6 Linear models","heading":"6.1 Introduction","text":"","code":"\ninstall.packages(\"DAAG\", repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggfortify\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"SMPracticals\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"reshape2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"tidyverse\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(DAAG)\nlibrary(ggfortify)\nlibrary(ggplot2)\nlibrary(SMPracticals)\nlibrary(reshape2)\nlibrary(tidyverse)"},{"path":"linear-models.html","id":"a-linear-model-analysis-for-comparing-groups","chapter":"6 Linear models","heading":"6.2 A linear model analysis for comparing groups","text":"general function creating linear models lm()’s good practice assign model object (.e. ls0)\nShort least squares model 0 linear model analysis uses technique known least squares\nShort least squares model 0 linear model analysis uses technique known least squaresCreate first model:ls0 model height relation 1 (another variable)comparisons made model intercept equals grand meanGenerate concise summary (table coefficients) key output linear models:display(ls0) output:\ncoeff.est = intercept (’s grand mean case)\ncoeff.se = standard error\nn = sample size\nk = # parameters estimated model\n(case ’s 1 since ’s just grand mean)\nresidual sd = SD heights 30 plants\nR-squared 0.00 = 0% variation data accounted \nexplanatory variable since explanatory variable, proportion 0%\ncoeff.est = intercept (’s grand mean case)coeff.se = standard errorn = sample sizek = # parameters estimated model\n(case ’s 1 since ’s just grand mean)residual sd = SD heights 30 plantsR-squared 0.00 = 0% variation data accounted \nexplanatory variable since explanatory variable, proportion 0%intercept grand mean Darwin maize data. Confirmation:Visualize data:Create model height pollination type:Summarize model ls1:display(ls1) output:\nNeed figure “Intercept” table coefficients\nlonger equal grand mean\n\ntwo levels encompasses explanatory variable “type”\nOne “self” listed typeSelf\nintercept must explanatory variable (cross)\nAlso, R lists levels alphabetical order (cross self)\n\n\ntable, coeff.est intercept average height cross pollinated progeny\ncoef.se Intercept SE mean\n\ncoeff.est typeSelf difference mean height cross pollinated self pollinated progeny (’s negative)\nhelp us answer question, difference plant height results different pollination treatments?\ncoeff.se typeSelf also something new:\n’s standard error difference\nVariation expected around difference\n\n\nlinear model calculates pooled variation across groups take advantage fuller sample (greater n)\nAssumes groups variance; assumption must checked\n\nresidual sd - square root residual error variance\nR-squared - 18% variation explained pollination variable\nNeed figure “Intercept” table coefficients\nlonger equal grand mean\nlonger equal grand meanThere two levels encompasses explanatory variable “type”\nOne “self” listed typeSelf\nintercept must explanatory variable (cross)\nAlso, R lists levels alphabetical order (cross self)\n\nOne “self” listed typeSelfSo intercept must explanatory variable (cross)\nAlso, R lists levels alphabetical order (cross self)\nAlso, R lists levels alphabetical order (cross self)table, coeff.est intercept average height cross pollinated progeny\ncoef.se Intercept SE mean\ncoef.se Intercept SE meanThe coeff.est typeSelf difference mean height cross pollinated self pollinated progeny (’s negative)help us answer question, difference plant height results different pollination treatments?\ncoeff.se typeSelf also something new:\n’s standard error difference\nVariation expected around difference\n\ncoeff.se typeSelf also something new:’s standard error difference\nVariation expected around difference\nVariation expected around differenceThe linear model calculates pooled variation across groups take advantage fuller sample (greater n)\nAssumes groups variance; assumption must checked\nAssumes groups variance; assumption must checkedresidual sd - square root residual error varianceR-squared - 18% variation explained pollination variableAnnotate mean scatterplot:","code":"\nls0 <- lm(formula = height ~1,  #the 1 indicates that we just want to\n                                  #estimate an intercept\n          data = darwin) \ndisplay(ls0) #from the arm package\n#> lm(formula = height ~ 1, data = darwin)\n#>             coef.est coef.se\n#> (Intercept) 18.88     0.58  \n#> ---\n#> n = 30, k = 1\n#> residual sd = 3.18, R-Squared = 0.00\nx<-format(round(mean(darwin$height), 2), nsmall = 2)\nx <- as.numeric(x)\nx\n#> [1] 18.88\nbase_plot <- ggplot(darwin, aes(x = type, y = height)) + geom_boxplot() +\n    theme_bw()\nbase_plot  # the marked points are outliers \nls1 <- lm(height ~ type, data = darwin) #does height vary as a function of type of pollination? \ndisplay(ls1)\n#> lm(formula = height ~ type, data = darwin)\n#>             coef.est coef.se\n#> (Intercept) 20.19     0.76  \n#> typeSelf    -2.62     1.07  \n#> ---\n#> n = 30, k = 2\n#> residual sd = 2.94, R-Squared = 0.18\nggplot(darwin, aes(x = type, y = height)) + geom_point() +  \n  stat_summary(fun = mean, #superimpose the mean on the plot \n               geom = \"point\", \n               colour = \"blue\", shape =8, size =5)"},{"path":"linear-models.html","id":"standard-error-of-the-difference","chapter":"6 Linear models","heading":"6.3 Standard error of the difference","text":"Standard error difference (SED) - standard error difference two means:subscripts indicate two groups (.e. treatment groups)","code":""},{"path":"linear-models.html","id":"confidence-intervals-1","chapter":"6 Linear models","heading":"6.4 Confidence intervals","text":"Calculate upper lower bounds CI model:Output:\nSimilar output display() function\nfirst row (Intercept) shows 95% CI height seed developed outcrossing\nsecond row shows 95% CI difference height two groups\nSimilar output display() functionThe first row (Intercept) shows 95% CI height seed developed outcrossingThe second row shows 95% CI difference height two groupsThe upper (97.5%) lower (2.5%) bounds percentiles normal distributionHelpful incorporate data one table:Darwin hypothesized progeny produced selfing lower fitness levels (reflected height)CIs can used figure reflected data","code":"\nCI_ls1<-confint(ls1)\nCI_ls1\n#>                2.5 %     97.5 %\n#> (Intercept) 18.63651 21.7468231\n#> typeSelf    -4.81599 -0.4173433"},{"path":"linear-models.html","id":"answering-darwins-question","chapter":"6 Linear models","heading":"6.5 Answering Darwin’s question","text":"null hypothesis experiment difference fitness (height) progeny produced selfing outcrossingA confidence interval test can used uphold reject null hypothesis\nrequires determine predicted null value (zero) lies inside confidence interval (see figure 5.3)\nzero lies outside bounds, null hypothesis can rejected confidence interval estimated mean difference (-2.62 inches) can distinguished null predicted value given variability data\nVariability data quantified standard error, used calculate CI\nrequires determine predicted null value (zero) lies inside confidence interval (see figure 5.3)zero lies outside bounds, null hypothesis can rejected confidence interval estimated mean difference (-2.62 inches) can distinguished null predicted value given variability dataVariability data quantified standard error, used calculate CIWe can visualize numberlinePlot number line difference means respective CI:\n(see guide)Number line conclusions:\nsince 0 outside 95% CI, can reject null hypothesis standard\nsince 0 outside 95% CI, can reject null hypothesis standardCreate model 99% CI requirement:Output:\nnote change sign upper bound typeSelf shows 0 now within CI\nnote change sign upper bound typeSelf shows 0 now within CIRedraw number line new model:Conclusions:\nreject null hypothesis 99% CI 0 within \nreject null hypothesis 99% CI 0 within ","code":"\nfigu6_4 <- coefplot(ls1, xlim = c(-5,0))\nconfint(ls1, level = 0.99)\n#>                 0.5 %     99.5 %\n#> (Intercept) 18.093790 22.2895433\n#> typeSelf    -5.583512  0.3501789\nfig6_5 <- coefplot(ls1,  \n                   sd = 3) #sd actually refers the # of standard errors here "},{"path":"linear-models.html","id":"relevelling-to-get-the-other-treatment-mean-and-standard-error","chapter":"6 Linear models","heading":"6.6 Relevelling to get the other treatment mean and standard error","text":"limitation methods table coefficients display() produce mean standard error another level\nseen, can get difference means SED\nseen, can get difference means SEDTo find mean standard error level(s), variables relevelledRelevel get mean standard error data selfed progeny population:Summarize model:standard error mean (Intercept$coef.se) typeSelf typeCross (0.76)\nBased formula SE, SE value depends \\(variance^2\\) sample size (n) group\nHowever, R’s lm() function uses single (pooled) estimate residual variance treatment levels (concept revisited ANOVA section)\nSEMs different treatments vary, must due sample size group differing\ncase Darwin’s maize data, group comprised 15 plants\nSince groups equal number biological reps lm() used, didn’t relevel groups already known SEMs groups \nBased formula SE, SE value depends \\(variance^2\\) sample size (n) groupHowever, R’s lm() function uses single (pooled) estimate residual variance treatment levels (concept revisited ANOVA section)SEMs different treatments vary, must due sample size group differingBut case Darwin’s maize data, group comprised 15 plantsSince groups equal number biological reps lm() used, didn’t relevel groups already known SEMs groups ","code":"\ndarwin$type <- relevel(darwin$type, #in this column:\n                       ref = \"Self\") #make the reference the Self group \ndisplay(lm(height ~type, data = darwin)) #now, Intercept's coef.est and coef.se refer to those of the selfed population \n#> lm(formula = height ~ type, data = darwin)\n#>             coef.est coef.se\n#> (Intercept) 17.58     0.76  \n#> typeCross    2.62     1.07  \n#> ---\n#> n = 30, k = 2\n#> residual sd = 2.94, R-Squared = 0.18"},{"path":"linear-models.html","id":"assumption-checking","chapter":"6 Linear models","heading":"6.7 Assumption checking","text":"objective assess assumptions unexplained variations two treatment groups approximately normal equal variabilityIt recommended plot residual differences assess thisResiduals differences observed values (heights) ‘fitted’ values predicted linear modelThis assumption approximate normality applies linear models use normal distribution model variabiliity\nmeasures precision (SEs) confidence (CIs) derived \nmeasures precision (SEs) confidence (CIs) derived fromThe assumption approximately equal variance follows use single pooled estimate variance across treatment groups","code":""},{"path":"linear-models.html","id":"normality","chapter":"6 Linear models","heading":"6.7.1 Normality","text":"Check assumptions using diagnostic plots using ggfortify -\nDisplay normal quantile (Q-Q) plot:\nDisplay normal quantile (Q-Q) plot:fig6_6 takeaways:\nplot graphs quantiles distribution residuals compared quantiles random sample normally distributed numbers\ndotted diagnonal line represents normal distribution\ndots fall around dotted line, closely match normal model\nlarge portion residuals approximately normally distributed\npoints deviate normal extremes\nsurprise already know outliers data\n\nOverall, assessment shows data ideally normal, appropriate still\nplot graphs quantiles distribution residuals compared quantiles random sample normally distributed numbersThe dotted diagnonal line represents normal distributionWhen dots fall around dotted line, closely match normal modelA large portion residuals approximately normally distributedThe points deviate normal extremes\nsurprise already know outliers data\nsurprise already know outliers dataOverall, assessment shows data ideally normal, appropriate still","code":"\nfig6_6 <- autoplot(ls1, \n                   which = c(2), #refers to which of the graphs you want displayed \n                                      #in this case it's it can be c(1,2,3,4) \n                   ncol = 1) #sets the orientation in which the plots are displayed \nfig6_6"},{"path":"linear-models.html","id":"equal-variance","chapter":"6 Linear models","heading":"6.7.2 Equal variance","text":"Two graphs assess variances treatment groups approximately equal:\nRaw residuals plot (left) - positive negative values\nStandarized residuals plot (right) - positive values\n\\(\\sqrt|\\frac{residuals}{residuals.SD}|\\)\n\nRaw residuals plot (left) - positive negative valuesStandarized residuals plot (right) - positive values\n\\(\\sqrt|\\frac{residuals}{residuals.SD}|\\)\n\\(\\sqrt|\\frac{residuals}{residuals.SD}|\\)fig6_7:\ngroup higher fitted values outcrossed group, higher mean height\nplots confirm impressions raw data (see base_plot) cross pollinated treatment data variable, mainly due outlier values\ngroup higher fitted values outcrossed group, higher mean heightThese plots confirm impressions raw data (see base_plot) cross pollinated treatment data variable, mainly due outlier values","code":"\nfig6_7 <- autoplot(ls1, \n                   which = c(1,3),\n                   ncol = 2)\nfig6_7"},{"path":"linear-models.html","id":"appendix-6b-robust-linear-models","chapter":"6 Linear models","heading":"Appendix 6b: Robust linear models","text":"Robust linear model function (rlm()):","code":"\nsummary(rlm(height ~ type, data = darwin)) \n#> \n#> Call: rlm(formula = height ~ type, data = darwin)\n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -9.1542 -1.4374  0.2501  0.9469  2.6251 \n#> \n#> Coefficients:\n#>             Value   Std. Error t value\n#> (Intercept) 17.7499  0.6362    27.8989\n#> typeCross    3.4043  0.8998     3.7836\n#> \n#> Residual standard error: 1.646 on 28 degrees of freedom"},{"path":"linear-models.html","id":"appendix-6c-exercise","chapter":"6 Linear models","heading":"Appendix 6c: Exercise","text":"Repeat analysis Darwin’s mignonette data…mignonette data wide form, want make pollination type variableAdd two variables pollination type:Combine type1 type2 variables one new column - type:Subset dataframe change one variable names:Create scatter plot:Change height numeric object:Use wide-format dataframe now calculation:Calculuate mean, SD, standard error:Create linear model:Summarize mig model:Calculate 95% CI model:Plot model number line:CI contain zero, can reject null hypothesis 95% CINow create model 99% CI:now 99% CI contain zero, fail reject NH levelCheck diagnostics, starting normal q-q plot:since points near dotted line, data seem fit model appropriatelyNext, residual variance plots:variances look comparable","code":"\nstr(mignonette)\n#> 'data.frame':    24 obs. of  2 variables:\n#>  $ cross: num  21 14.2 19.1 7 15.1 ...\n#>  $ self : num  12.9 16 11.9 15.2 19.1 ...\nmignonette$type1 <- \"type_cross\"\nmignonette$type2 <- \"type_self\"\nhead(mignonette)\n#>    cross   self      type1     type2\n#> 1 21.000 12.875 type_cross type_self\n#> 2 14.250 16.000 type_cross type_self\n#> 3 19.125 11.875 type_cross type_self\n#> 4  7.000 15.250 type_cross type_self\n#> 5 15.125 19.125 type_cross type_self\n#> 6 20.500 12.500 type_cross type_self\ndf.new<-melt(mignonette, id.var = c('type1','type2'), variable.name = 'type')\nhead(df.new) \n#>        type1     type2  type  value\n#> 1 type_cross type_self cross 21.000\n#> 2 type_cross type_self cross 14.250\n#> 3 type_cross type_self cross 19.125\n#> 4 type_cross type_self cross  7.000\n#> 5 type_cross type_self cross 15.125\n#> 6 type_cross type_self cross 20.500\nmignonette2 <- subset(select(df.new, -type1 & -type2))\nhead(mignonette2)\n#>    type  value\n#> 1 cross 21.000\n#> 2 cross 14.250\n#> 3 cross 19.125\n#> 4 cross  7.000\n#> 5 cross 15.125\n#> 6 cross 20.500\n\nmignonette2$height <- mignonette2$value\nhead(mignonette2)\n#>    type  value height\n#> 1 cross 21.000 21.000\n#> 2 cross 14.250 14.250\n#> 3 cross 19.125 19.125\n#> 4 cross  7.000  7.000\n#> 5 cross 15.125 15.125\n#> 6 cross 20.500 20.500\nbase_plot<-ggplot(mignonette2, aes(x = type, y = height)) + \n  geom_boxplot() +                    \n  theme_bw()\nbase_plot # there are no outliers\nmignonette2$height <- as.numeric(mignonette2$height) \nstr(mignonette2)\n#> 'data.frame':    48 obs. of  3 variables:\n#>  $ type  : Factor w/ 2 levels \"cross\",\"self\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ value : num  21 14.2 19.1 7 15.1 ...\n#>  $ height: num  21 14.2 19.1 7 15.1 ...\nmignonette$difference <- mignonette$cross - mignonette$self \nmig.mean<-mean(mignonette$difference) \nmig.sd<-sd(mignonette$difference)\nmig.se <- mig.sd/sqrt(12)\nmig_ls <- lm(value ~ type,data = mignonette2)\ndisplay(mig_ls)\n#> lm(formula = value ~ type, data = mignonette2)\n#>             coef.est coef.se\n#> (Intercept) 17.18     0.82  \n#> typeself    -2.56     1.16  \n#> ---\n#> n = 48, k = 2\n#> residual sd = 4.00, R-Squared = 0.10\nconfint(mig_ls)\n#>                 2.5 %     97.5 %\n#> (Intercept) 15.531838 18.8223283\n#> typeself    -4.884019 -0.2305639\nmig_NL <- coefplot(mig_ls, xlim = c(-5,0)) \nconfint(mig_ls, level = 0.99)\n#>                0.5 %     99.5 %\n#> (Intercept) 14.98085 19.3733201\n#> typeself    -5.66324  0.5486562\nautoplot(mig_ls, which = c(2), ncol=1)\nautoplot(mig_ls, which = c(1,3), ncol=2)"},{"path":"regression.html","id":"regression","chapter":"7 Regression","heading":"7 Regression","text":"","code":""},{"path":"regression.html","id":"introduction-4","chapter":"7 Regression","heading":"7.1 Introduction","text":"","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"MASS\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"SemiPar\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"agricolae\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggfortify\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(MASS)\nlibrary(SemiPar)\nlibrary(agricolae)\nlibrary(ggfortify)"},{"path":"regression.html","id":"linear-regression","chapter":"7 Regression","heading":"7.2 Linear regression","text":"Linear regression\nUsed define relationship response explanatory variable\nscatter plot used draw straight line model mean\nVariability around mean captured size standard errors\nreflected confidence intervals calculated \n\nuse \\(y = b + mx\\)\n\\(y\\) = predicted value response variable (.e. hardness janka data)\n\\(b\\) = regression intercept (\\(y\\) \\(x = 0\\))\n\\(m\\) = slope regression line\n\\(x\\) = value explanatory variable (.e. density janka data)\n\nUnexplained residual (error) variation can added additional term, \\(e\\)\n\\(y = b + mx + e\\)\n\nUsed define relationship response explanatory variableA scatter plot used draw straight line model meanVariability around mean captured size standard errors\nreflected confidence intervals calculated \nreflected confidence intervals calculated themWe use \\(y = b + mx\\)\n\\(y\\) = predicted value response variable (.e. hardness janka data)\n\\(b\\) = regression intercept (\\(y\\) \\(x = 0\\))\n\\(m\\) = slope regression line\n\\(x\\) = value explanatory variable (.e. density janka data)\n\\(y\\) = predicted value response variable (.e. hardness janka data)\\(b\\) = regression intercept (\\(y\\) \\(x = 0\\))\\(m\\) = slope regression line\\(x\\) = value explanatory variable (.e. density janka data)Unexplained residual (error) variation can added additional term, \\(e\\)\n\\(y = b + mx + e\\)\n\\(y = b + mx + e\\)","code":""},{"path":"regression.html","id":"janka-timber-hardness-data","chapter":"7 Regression","heading":"7.3 Janka timber hardness data","text":"goal collecting data establish linear relationship wood density hardnessDensity easier measure hardness testing density can used proxyThis dataset includes 36 samples wood unique tree speciesOnce linear relationship established, model can used predict timber hardness wood known density valueData found SemiPar package dataframe, jankaNeed load data semipar package:Visualize data - Wood hardness function density - via scatterplot:positive relationship relationship wood hardness density seems linear","code":"\ndata(janka)\nhead(janka)\n#>   dens hardness\n#> 1 24.7      484\n#> 2 24.8      427\n#> 3 27.3      413\n#> 4 28.4      517\n#> 5 28.4      549\n#> 6 29.0      648\nstr(janka)\n#> 'data.frame':    36 obs. of  2 variables:\n#>  $ dens    : num  24.7 24.8 27.3 28.4 28.4 29 30.3 32.7 35.6 38.5 ...\n#>  $ hardness: int  484 427 413 517 549 648 587 704 979 914 ...\nxlabel <- expression(paste(\"Wood density (lbs • \", ft^-3, \")\")) #expression allows you to modify certain parts of the text of the label\nylabel <- \"Timber hardness (lbf)\"\nfig7_1 <- ggplot(janka, aes(x = dens, y = hardness)) + geom_point() +\n    xlab(xlabel) + ylab(ylabel) + theme_bw()\nfig7_1"},{"path":"regression.html","id":"correlation","chapter":"7 Regression","heading":"7.4 Correlation","text":"Test strength correlation:value close +1; therefore, strong positive linear relationship two variablesWe can go beyond correlation argue higher wood density causes higher timber hardness using linear regression\nMathematically, say changes \\(x\\) lead changes \\(y\\)\nMathematically, say changes \\(x\\) lead changes \\(y\\)","code":"\nwith(janka, cor(dens, hardness))\n#> [1] 0.9743345"},{"path":"regression.html","id":"linear-regression-in-r","chapter":"7 Regression","heading":"7.5 Linear regression in R","text":"data similiar Darwin data except explanatory variable, density, continuousCreate linear model:model estimates ‘line best fit’ (regression line) using method least squares minimize error sums squares, average distance datapoints line (see appendix 7b)Add linear model regression line (95% CI band) scatter plot:Figure takeaways:\nCI curved upper lower bounds narrowest middle\nresults calculated standard errors regression intercept regression slope\nuncertainty elevation line gradient\nCI curved upper lower bounds narrowest middleThis results calculated standard errors regression intercept regression slopeThere uncertainty elevation line gradientCheck model’s estimates coefficients:display(janka.ls1) takeaways:Intercept refers hardness variable\nIntercept’s coef.est (\\(y = -1160.50\\) \\(x (density) = 0\\)\nSide note: ’s possible negative hardness density zero, somewhat artifact analysis\n\ncoef.se standard errorr regression intercept\nOne thing get meaning data technique called centering\n\nCentering means subtracting average density value (x) individual value\nuseful strategy provide value response average value explanatory variable\nIntercept’s coef.est (\\(y = -1160.50\\) \\(x (density) = 0\\)\nSide note: ’s possible negative hardness density zero, somewhat artifact analysis\nSide note: ’s possible negative hardness density zero, somewhat artifact analysiscoef.se standard errorr regression intercept\nOne thing get meaning data technique called centering\nOne thing get meaning data technique called centeringCentering means subtracting average density value (x) individual valueThis useful strategy provide value response average value explanatory variabledens = 57.51 value regression slope\nincrease 1 unit wood density means increase timber hardness 57.51 units\nincrease 1 unit wood density means increase timber hardness 57.51 unitsn = sample sizek = number estimated parameters (regression intercept slope)R-squared = proportion variation data explained linear regression analysisFind CI (95%) upper lower bounds:","code":"\njanka.ls1 <- lm(hardness ~ dens, data = janka)\nfig7_2 <- fig7_1 +\n  geom_smooth(method = \"lm\")  #fit a linear model regression line to the data points \nfig7_2\n#> `geom_smooth()` using formula 'y ~ x'\ndisplay(janka.ls1)\n#> lm(formula = hardness ~ dens, data = janka)\n#>             coef.est coef.se \n#> (Intercept) -1160.50   108.58\n#> dens           57.51     2.28\n#> ---\n#> n = 36, k = 2\n#> residual sd = 183.06, R-Squared = 0.95\n  confint(janka.ls1)\n#>                   2.5 %     97.5 %\n#> (Intercept) -1381.16001 -939.83940\n#> dens           52.87614   62.13721"},{"path":"regression.html","id":"assumptions","chapter":"7 Regression","heading":"7.6 Assumptions","text":"linear model makes assumptions data seen including:\nassumption unexplained variability around regression line (residuals) approximately normal constant variance\nresiduals differences observed observed datapoints fitted values predicted model\nResidual differences vertical distances datapoints corresponding points regression line (fitted values)\nResiduals represented red lines Figure 7.3\n\nassumption unexplained variability around regression line (residuals) approximately normal constant varianceThe residuals differences observed observed datapoints fitted values predicted model\nResidual differences vertical distances datapoints corresponding points regression line (fitted values)\nResiduals represented red lines Figure 7.3\nResidual differences vertical distances datapoints corresponding points regression line (fitted values)Residuals represented red lines Figure 7.3Display (two ) residuals graphically fitted regression data:Figure 7.3 takeaways:\ngraph, see two residuals, vertical differences\ndatapoint corresponding fitted value x value\n\nlinear model analysis finds regression ‘line best fit’ using method least squares minimize average distance datapoints line (unexplained variation\nimagine red line datapoint blue line\nvertical distances used best fit calculation\n\nmethod discussed chapter 11\n\ngraph, see two residuals, vertical differences\ndatapoint corresponding fitted value x value\ndatapoint corresponding fitted value x valueThe linear model analysis finds regression ‘line best fit’ using method least squares minimize average distance datapoints line (unexplained variation\nimagine red line datapoint blue line\nvertical distances used best fit calculation\n\nmethod discussed chapter 11\nimagine red line datapoint blue line\nvertical distances used best fit calculation\nvertical distances used best fit calculationThis method discussed chapter 11Extract fitted values predicted linear model:Extract residuals:Check normality data:figure shows data fairly normally distributedoutliers individually labelled plot\nvalues much bigger value normal dataset\nvalues much bigger value normal datasetCheck variability data:Figure 7.5 takeaways:\nLeft - raw residuals function fitted values\nRight -\nDisplays absolute value residuals (|residuals|)\nStandardizes :\n\n\\(\\sqrt\\frac{residual}{residualSD}\\)\n\n\nplots suggest residuals don’t constant variance\nvariability seems increase mean increases\nlater chapters, see appropriate model data\n\nLeft - raw residuals function fitted valuesRight -\nDisplays absolute value residuals (|residuals|)\nStandardizes :\n\n\\(\\sqrt\\frac{residual}{residualSD}\\)\n\nDisplays absolute value residuals (|residuals|)Standardizes :\n\n\\(\\sqrt\\frac{residual}{residualSD}\\)\nplots suggest residuals don’t constant variance\nvariability seems increase mean increases\nlater chapters, see appropriate model data\nvariability seems increase mean increasesIn later chapters, see appropriate model data","code":"\nfig7_3 <- \n  fig7_1 + geom_smooth(method = \"lm\", \n                       se = FALSE) + \n  geom_segment(aes(x = 66, xend = 66, y =3250, yend = 2650), color = \"red\") + \n    geom_segment(aes(x = 59.8, xend = 59.8, y =1955, yend = 2265), color = \"red\") \nfig7_3\n#> `geom_smooth()` using formula 'y ~ x'\nfitted(janka.ls1)\n#>         1         2         3         4         5         6 \n#>  259.9152  265.6658  409.4325  472.6899  472.6899  507.1939 \n#>         7         8         9        10        11        12 \n#>  581.9525  719.9686  886.7379 1053.5073 1070.7593 1099.5126 \n#>        13        14        15        16        17        18 \n#> 1105.2633 1134.0166 1157.0193 1174.2713 1180.0220 1180.0220 \n#>        19        20        21        22        23        24 \n#> 1306.5366 1473.3060 1536.5633 1611.3220 1801.0940 1801.0940 \n#>        25        26        27        28        29        30 \n#> 1910.3567 2059.8741 2088.6274 2134.6328 2151.8848 2243.8954 \n#>        31        32        33        34        35        36 \n#> 2278.3994 2634.9408 2715.4502 2795.9595 2813.2115 2813.2115\nhead(residuals(janka.ls1))\n#>          1          2          3          4          5 \n#> 224.084837 161.334170   3.567483  44.310140  76.310140 \n#>          6 \n#> 140.806135\ntail(residuals(janka.ls1)) #displays the last 6 rows in the dataframe \n#>         31         32         33         34         35 \n#> -338.39945  625.05917  -15.45018   94.04048  -73.21152 \n#>         36 \n#>  326.78848\nfig7_4 <- autoplot(janka.ls1, which = c(2), ncol =1)\nfig7_4\nfig7_5 <- autoplot(janka.ls1, which = c(1,3), ncol =2)\nfig7_5"},{"path":"prediction.html","id":"prediction","chapter":"8 Prediction","heading":"8 Prediction","text":"","code":""},{"path":"prediction.html","id":"introduction-5","chapter":"8 Prediction","heading":"8.1 Introduction","text":"previous section, linear regression tot model relationship wood density timber hardness establishedThe coefficients linear model - regression intercept slop - foundIn section, coefficients used predict timber hardness new density values","code":"\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"dplyr\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"SemiPar\",  repos = \"https://cran.us.r-project.org\")\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(SemiPar)"},{"path":"prediction.html","id":"predicting-timber-hardness-from-wood-density","chapter":"8 Prediction","heading":"8.2 Predicting timber hardness from wood density","text":"Predicting wood hardness known density valueRather using \\(y = mx + b\\) solve y, can use tools RThe model created last section data encompasses density values 20-70 lbs/\\(ft^3\\) correspondingly, 3000 units janka timber hardness scaleReload janka data:estimates intercept slope :find timber hardness density value 65:numbers brackets refer coefficent\n1 refers intercept\n2 refers slope\n1 refers intercept2 refers slopeWe can accomplish thing predict function:default, prediction made 36 density values:fitted values (predictions) saw last chapter - data points regression line\ndensity (x) values fed model (\\(y = b + mx\\) equation model) output fitted predicted values\ndensity (x) values fed model (\\(y = b + mx\\) equation model) output fitted predicted valuesR calculates fitted values combining estimates coefficents model matrix (X-matrix)Take look model matrix:(Intercept), , refers (response) variable, case, hardness\njust like seen display(janka.ls1)\ncolumn 1s include value rarely see model formulas:\n\nmodel formula janka.ls1 written can also written :just like seen display(janka.ls1)column 1s include value rarely see model formulas::\n\\(y = b\\cdot1 + m\\cdot x\\)\n, intercept, b, multiplied 1\n\n\\(y = b\\cdot1 + m\\cdot x\\)\n, intercept, b, multiplied 1\n, intercept, b, multiplied 1To calculate predicted hardness wood sample lowest density, take estimated values intercept slope:combine values first row model matrix:like :translates \\(b\\cdot1 + m\\cdot x = 259.915\\) :\n\\(b\\) = -1160.49970\n\\(m\\) = 57.50667\n\\(x\\) = 24.7\n\\(b\\) = -1160.49970\\(m\\) = 57.50667\\(x\\) = 24.7R figures easily (precisely without rounding error):","code":"\ndata(janka)\njanka.ls1 <- lm(hardness ~ dens, data = janka)\ncoef(janka.ls1)\n#> (Intercept)        dens \n#> -1160.49970    57.50667\ncoef(janka.ls1)[1] + coef(janka.ls1)[2] *65 #b + mx = timber hardness value \n#> (Intercept) \n#>    2577.434\npredict(object = janka.ls1, #the model used to make the prediction \n        newdata = list(dens=65)) #the value(s) we want to make the prediction for \n#>        1 \n#> 2577.434\npredict(object = janka.ls1)\n#>         1         2         3         4         5         6 \n#>  259.9152  265.6658  409.4325  472.6899  472.6899  507.1939 \n#>         7         8         9        10        11        12 \n#>  581.9525  719.9686  886.7379 1053.5073 1070.7593 1099.5126 \n#>        13        14        15        16        17        18 \n#> 1105.2633 1134.0166 1157.0193 1174.2713 1180.0220 1180.0220 \n#>        19        20        21        22        23        24 \n#> 1306.5366 1473.3060 1536.5633 1611.3220 1801.0940 1801.0940 \n#>        25        26        27        28        29        30 \n#> 1910.3567 2059.8741 2088.6274 2134.6328 2151.8848 2243.8954 \n#>        31        32        33        34        35        36 \n#> 2278.3994 2634.9408 2715.4502 2795.9595 2813.2115 2813.2115\nhead(model.matrix(janka.ls1))\n#>   (Intercept) dens\n#> 1           1 24.7\n#> 2           1 24.8\n#> 3           1 27.3\n#> 4           1 28.4\n#> 5           1 28.4\n#> 6           1 29.0\ntail(model.matrix(janka.ls1))\n#>    (Intercept) dens\n#> 31           1 59.8\n#> 32           1 66.0\n#> 33           1 67.4\n#> 34           1 68.8\n#> 35           1 69.1\n#> 36           1 69.1\njanka.ls1 <- lm(formula = 1 + #shows that 1 is factored into the model as a starting point for the model calculation \n                  hardness ~ dens, data = janka)\ncoef(janka.ls1)\n#> (Intercept)        dens \n#> -1159.49970    57.50667\nmodel.matrix(janka.ls1)[1,]\n#> (Intercept)        dens \n#>         1.0        24.7\n-1160.49970*1 +57.50667*24.7\n#> [1] 259.915\ncoef(janka.ls1)[1] * model.matrix(janka.ls1)[1,1] +\ncoef(janka.ls1)[2] * model.matrix(janka.ls1)[1,2]\n#> (Intercept) \n#>    260.9152"},{"path":"prediction.html","id":"confidence-intervals-and-prediction-intervals","chapter":"8 Prediction","heading":"8.3 Confidence intervals and prediction intervals","text":"’s easy plot data regression line plus 95% CI:Remember 95% CI reflects confidence average relationship - regression line (conveys inferential uncertainty)Now want use relationship predict new y values (hardness) new wood samples based x values (density)need accompany prediction interval shows confidence \n95% CI achieve \n95% CI achieve thisInstead, use prediction interval (PI), conveys inferential uncertainty shows confidence estimate regression relationshipThere scatter around regression line inherently uncertainty prediction new y values based corresponding x values (predictive uncertainty)PI quantified residual variance (residual error mean square)predict() functiton provide SE, upper/lower bounds confidence interval, upper lower bounds prediction interval (PI) (95% PI default)Calculate point estimate based x (density) value:Now point estimate 95% CI:Finally, prediction hardness based density value 95% prediction interval:process generating predicted values corresponding PIs can extended range x valuesBut 36 density values evenly spreadWe need generate longer, regular sequence x values (.e. 100 equally spaced values) get smooth intervalCreate sequence:Use newly generated sequence create dataframe new predicted values respective PIs:Add x values dataframe:allows us plot values x y plotRename dataframe variable names familiar ones:Redraw figure CI (blue) PI (grey):major takeaway: PI much wider CI\ndatapoints make narrow\ndatapoints make narrow","code":"\nFig8_1 <- ggplot(janka, aes(x=dens, y = hardness)) + geom_point() +\n  geom_smooth(method = \"lm\")\nFig8_1\n#> `geom_smooth()` using formula 'y ~ x'\npredict(janka.ls1, newdata = list(dens=65),\n        se = TRUE) #display standard error \n#> $fit\n#>        1 \n#> 2578.434 \n#> \n#> $se.fit\n#> [1] 53.46068\n#> \n#> $df\n#> [1] 34\n#> \n#> $residual.scale\n#> [1] 183.0595\npredict(janka.ls1, newdata = list(dens=65), interval = \"confidence\") \n#>        fit      lwr      upr\n#> 1 2578.434 2469.789 2687.079\npredict(janka.ls1, newdata = list(dens=65), interval = \"predict\")\n#>        fit      lwr      upr\n#> 1 2578.434 2190.873 2965.996\nxseq <- seq(from = min(janka$dens), \n            to = max(janka$dens), \n            length.out = 100) #number of values between the min and max parameters \nprediction_interval <- \npredict(janka.ls1, newdata = list(dens = xseq), interval = \"predict\")\nhead(prediction_interval)\n#>        fit         lwr      upr\n#> 1 260.9152 -128.610798 650.4411\n#> 2 286.7060 -102.305865 675.7179\n#> 3 312.4969  -76.011353 701.0052\n#> 4 338.2878  -49.727303 726.3029\n#> 5 364.0787  -23.453754 751.6111\n#> 6 389.8695    2.809255 776.9298\nfig_data <- data.frame(xseq, prediction_interval)\nhead(fig_data)\n#>       xseq      fit         lwr      upr\n#> 1 24.70000 260.9152 -128.610798 650.4411\n#> 2 25.14848 286.7060 -102.305865 675.7179\n#> 3 25.59697 312.4969  -76.011353 701.0052\n#> 4 26.04545 338.2878  -49.727303 726.3029\n#> 5 26.49394 364.0787  -23.453754 751.6111\n#> 6 26.94242 389.8695    2.809255 776.9298\nfig_data <- rename(fig_data, dens = xseq, hardness = fit)\nhead(fig_data)\n#>       dens hardness         lwr      upr\n#> 1 24.70000 260.9152 -128.610798 650.4411\n#> 2 25.14848 286.7060 -102.305865 675.7179\n#> 3 25.59697 312.4969  -76.011353 701.0052\n#> 4 26.04545 338.2878  -49.727303 726.3029\n#> 5 26.49394 364.0787  -23.453754 751.6111\n#> 6 26.94242 389.8695    2.809255 776.9298\nFig8_2 <- ggplot(janka, aes(x = dens, y =hardness)) + geom_point() + \n  geom_smooth(data = janka, method = \"lm\", fill = \"blue\") + \n  geom_smooth(data = fig_data, aes(ymin = lwr, ymax = upr), stat = \"identity\")\nFig8_2\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"testing.html","id":"testing","chapter":"9 Testing","heading":"9 Testing","text":"","code":""},{"path":"testing.html","id":"significance-testing-time-for-t","chapter":"9 Testing","heading":"9.1 Significance testing: Time for t","text":"Statisticians think overfixation p-values contributed reproducibility crisis scienceThat’s author’s focus text estimation, relies estimates confidence intervalsThis chapter introduces Student’s t-test generate p-value","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"Sleuth2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"SMPracticals\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(Sleuth2)\nlibrary(SMPracticals)"},{"path":"testing.html","id":"students-t-test-darwins-maize","chapter":"9 Testing","heading":"9.2 Student’s t-test: Darwin’s maize","text":"Student’s t-test\nUses t-distribution\nlike small sample size version normal distribution\n\ntwo basic forms t-test can take:\none sample t-test takes mean sample compares null hypothesis zero\ntwo sample t-test compares difference means two samples null hypothesis difference (zero)\npaired two sample t-test subtype applies values two samples come pairs (like darwin maize data)\n\n\nR function t-test author tends avoid . Criticism author default R t-test:\nparts t-test include:\ndifference\nstandard error (CI used calculate )\nobserved value t\ncritical value t\ndegrees freedom\nP-value\n\ndefault R t-test output poorly ordered missing information\ndefault classic t-test another variant called Welch’s\nWelch’s might good research since doesn’t assume equal variance though\n\nadds another twist ’s appropriate teaching beginners\n\nUses t-distribution\nlike small sample size version normal distribution\nlike small sample size version normal distributionThere two basic forms t-test can take:\none sample t-test takes mean sample compares null hypothesis zero\ntwo sample t-test compares difference means two samples null hypothesis difference (zero)\npaired two sample t-test subtype applies values two samples come pairs (like darwin maize data)\n\none sample t-test takes mean sample compares null hypothesis zeroThe two sample t-test compares difference means two samples null hypothesis difference (zero)\npaired two sample t-test subtype applies values two samples come pairs (like darwin maize data)\npaired two sample t-test subtype applies values two samples come pairs (like darwin maize data)R function t-test author tends avoid . Criticism author default R t-test:\nparts t-test include:\ndifference\nstandard error (CI used calculate )\nobserved value t\ncritical value t\ndegrees freedom\nP-value\n\ndefault R t-test output poorly ordered missing information\ndefault classic t-test another variant called Welch’s\nWelch’s might good research since doesn’t assume equal variance though\n\nadds another twist ’s appropriate teaching beginners\nparts t-test include:\ndifference\nstandard error (CI used calculate )\nobserved value t\ncritical value t\ndegrees freedom\nP-value\ndifferenceits standard error (CI used calculate )observed value tthe critical value tthe degrees freedomthe P-valuethe default R t-test output poorly ordered missing informationthe default classic t-test another variant called Welch’s\nWelch’s might good research since doesn’t assume equal variance though\nWelch’s might good research since doesn’t assume equal variance thoughbut adds another twist ’s appropriate teaching beginnersthe general form t-test :\\(observed \\;t = \\frac{difference}{SE}\\)formal version done earlier chapters output display() function - estimates standard errors comparedas rule thumb/eyeball test, estimate twice large standard error reject null hypothesis lowest level confidence (95%) three times large next level (99% CI) onwhen sample sizes larger, t converges normal distributionwhen sample sizes smaller, t distribution shorter wider normaltwo t values t-test:\ncritical t value - sets bar comparison - minimum t value required achieve given level significance (P = 0.05, 0.01, etc.)\nobserved t value - calculated dividing estimate standard error\nobserved t value larger critical t value, result declared significant level\n\ncritical t value - sets bar comparison - minimum t value required achieve given level significance (P = 0.05, 0.01, etc.)observed t value - calculated dividing estimate standard error\nobserved t value larger critical t value, result declared significant level\nobserved t value larger critical t value, result declared significant levelFor performing paired t-test darwin’s maize data, observed t value must compared critical value corresponding critical t value sample size Darwin (\\(n = 15 \\;pairs\\))Calculate critical t value P = 0.05 n = 15 pairs:Summarize model version maize data omits pairing aspect (standard two sample t-test):summary() takeaways:\nsecond row coefficients table tests null hypothesis comparing observed difference height progeny selfed cross pollinated plants (-2.6167)\nfirst row tests mean height cross pollinated plants versus null hypothesis average height zero\ncomparison intended make advance really \n\nsecond row coefficients table tests null hypothesis comparing observed difference height progeny selfed cross pollinated plants (-2.6167)first row tests mean height cross pollinated plants versus null hypothesis average height zero\ncomparison intended make advance really \ncomparison intended make advance really allIt can beneficial specify tests want run even means typingThe average difference height -2.6167 standard error difference 1.0737This produces observed t value :critical t value data 2.144787observed t value data -2.437087It better/informative work CIs:, 95% CI encompass 0; therefore, null hypothesis can rejected levelIn previous summary() chunk, standard two sample t-test performed without factoring pairingGenerate table coefficients paired t-test:New table coefficients output:\nTakes crossed plant pair 1 intercept\nShows mean difference height crossed plants (-2.6167)\nshows average difference pair relative pair 1\nTakes crossed plant pair 1 interceptShows mean difference height crossed plants (-2.6167)shows average difference pair relative pair 1Ignore differences pairs see second row gives us t value paired t-test:observed t value, way smaller critical t value (2.144787)finding corresponds large p value table 0.6434We can use linear model function perform equivalent one-sample t test:\n- generate single sample differences:Fit linear model estimates mean difference:","code":"\nqt(0.975, df = 14)  #mean + 2SEs; 0.975 is the upper CI limit for a 95% CI \n#> [1] 2.144787\nsummary(\n  lm(height ~ type, data = darwin)) #this linear model omits the pairing aspect \n#> \n#> Call:\n#> lm(formula = height ~ type, data = darwin)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -8.1917 -1.0729  0.8042  1.9021  3.3083 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  20.1917     0.7592  26.596   <2e-16 ***\n#> typeSelf     -2.6167     1.0737  -2.437   0.0214 *  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.94 on 28 degrees of freedom\n#> Multiple R-squared:  0.175,  Adjusted R-squared:  0.1455 \n#> F-statistic:  5.94 on 1 and 28 DF,  p-value: 0.02141\n-2.6167/1.0737 #difference/standard error of the difference \n#> [1] -2.437087\nconfint(lm(height ~ type, data = darwin))\n#>                2.5 %     97.5 %\n#> (Intercept) 18.63651 21.7468231\n#> typeSelf    -4.81599 -0.4173433\nsummary(lm(height ~ type + pair, data = darwin))\n#> \n#> Call:\n#> lm(formula = height ~ type + pair, data = darwin)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -5.4958 -0.9021  0.0000  0.9021  5.4958 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  21.7458     2.4364   8.925 3.75e-07 ***\n#> typeSelf     -2.6167     1.2182  -2.148   0.0497 *  \n#> pair2        -4.2500     3.3362  -1.274   0.2234    \n#> pair3         0.0625     3.3362   0.019   0.9853    \n#> pair4         0.5625     3.3362   0.169   0.8685    \n#> pair5        -1.6875     3.3362  -0.506   0.6209    \n#> pair6        -0.3750     3.3362  -0.112   0.9121    \n#> pair7        -0.0625     3.3362  -0.019   0.9853    \n#> pair8        -2.6250     3.3362  -0.787   0.4445    \n#> pair9        -3.0625     3.3362  -0.918   0.3742    \n#> pair10       -0.6250     3.3362  -0.187   0.8541    \n#> pair11       -0.6875     3.3362  -0.206   0.8397    \n#> pair12       -0.9375     3.3362  -0.281   0.7828    \n#> pair13       -3.0000     3.3362  -0.899   0.3837    \n#> pair14       -1.1875     3.3362  -0.356   0.7272    \n#> pair15       -5.4375     3.3362  -1.630   0.1254    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.336 on 14 degrees of freedom\n#> Multiple R-squared:  0.469,  Adjusted R-squared:  -0.09997 \n#> F-statistic: 0.8243 on 15 and 14 DF,  p-value: 0.6434\n-2.6167/1.2182\n#> [1] -2.148005\nex0428$Difference <- ex0428$Cross - ex0428$Self\nsummary(lm(Difference ~1, #the 1 indicates the intercept, which is the mean of a single sample of differences\n           data = ex0428))\n#> \n#> Call:\n#> lm(formula = Difference ~ 1, data = ex0428)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -10.9917  -1.2417   0.3833   3.0083   6.7583 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)  \n#> (Intercept)    2.617      1.218   2.148   0.0497 *\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.718 on 14 degrees of freedom"},{"path":"testing.html","id":"summary-statistics-1","chapter":"9 Testing","heading":"9.3 Summary: statistics","text":"Alternate source t-testsI probably rarely ever use one sample t-testuse two-tailed t-test since don’t need determine direction differenceuse paired t-test groups come single population\n.e. measuring gene expression two groups cannabis plants time /methyl jasmonate treatment\n.e. measuring gene expression two groups cannabis plants time /methyl jasmonate treatmentuse two sample (unpaired) t-test two groups come different populations\n.e. comparing feature two varieties P. cubensis\ndifferent, CI include 0 null hypothesis can rejected\n.e. comparing feature two varieties P. cubensisif different, CI include 0 null hypothesis can rejected","code":""},{"path":"intervals.html","id":"intervals","chapter":"10 Intervals","heading":"10 Intervals","text":"","code":""},{"path":"intervals.html","id":"comparisons-usings-estimates-and-intervals","chapter":"10 Intervals","heading":"10.1 Comparisons usings estimates and intervals","text":"One action combat reproducibility crisis science prefer estimation based approach focus p-valuesThis section gives overview intervals can applied linear model analysis","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"cowplot\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"Sleuth2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"SMPracticals\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"patchwork\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(cowplot)\nlibrary(ggplot2)\nlibrary(Sleuth2)\nlibrary(SMPracticals)\nlibrary(patchwork)"},{"path":"intervals.html","id":"estimation-based-analysis","chapter":"10 Intervals","heading":"10.2 Estimation based analysis","text":"Author’s argument estimation-based analysis:\nMaking conclusions using significance testing approach based signal:noise ratio ANOVA table F-tests summary table t-tests whether ratios pass arbitrarily set level statistical significance\nWhereas, estimates intervals original scale data keep understanding closer data questions \nhand, different types error bars intervals can confusing\nDividing statistics two classes: descriptive inferential\nmany examples rest book two assumptions made data:\nLarge sample sizes t-distribution assumes normal distribution\nSamples sizes equal treatments\nSimplifies relationship standard error mean standard error difference\n\n\nMaking conclusions using significance testing approach based signal:noise ratio ANOVA table F-tests summary table t-tests whether ratios pass arbitrarily set level statistical significanceWhereas, estimates intervals original scale data keep understanding closer data questions themOn hand, different types error bars intervals can confusingDividing statistics two classes: descriptive inferentialIn many examples rest book two assumptions made data:\nLarge sample sizes t-distribution assumes normal distribution\nSamples sizes equal treatments\nSimplifies relationship standard error mean standard error difference\n\nLarge sample sizes t-distribution assumes normal distributionSamples sizes equal treatments\nSimplifies relationship standard error mean standard error difference\nSimplifies relationship standard error mean standard error difference","code":""},{"path":"intervals.html","id":"descriptive-statistics","chapter":"10 Intervals","heading":"10.3 Descriptive statistics","text":"Chapter 3 introduced different descriptive approaches looking Darwin’s maize data \nPlotting individual datapoints dataset small number datapoints\nUsing boxplot much greater number datapoints\nPlotting individual datapoints dataset small number datapointsUsing boxplot much greater number datapoints","code":""},{"path":"intervals.html","id":"error-bars","chapter":"10 Intervals","heading":"10.3.1 Error bars","text":"Another common type graph presents measure central tendancy (mean)\noften comes form bar plot, often misleading\nAuthor argues better plot datapoints, indicate mean unique symbol, show error bars variability\noften comes form bar plot, often misleadingAuthor argues better plot datapoints, indicate mean unique symbol, show error bars variabilityDescriptive statistics continuous data (.e. Darwin’s maize data) mean standard deviationAn alternative approach showing means SDs linear model analysis calculates ‘pooled’ estimate residual variability across treatments\ncan used instead calculating separate SDs different groups done previously maize data\ncan used instead calculating separate SDs different groups done previously maize dataWe going calculate intervals use geom_linerange() display \ngeom_linerange can used error bar interval\ngeom_linerange can used error bar intervalFirst, re-fit linear model maize data, provide estimate pooled SD:Using model, draw variety graphs showcase pollination means error bars different calculated intervalsSet dataframe hold calculated value later plot:Add height means corresponding levels:Generate pooled standard deviation want display error bar:pooled standard deviation called “sigma.hat”:Add sigma.hat value estimates table:adds value pooled variation treatmentsPlot values estimates corresponding pooled residual variation error bars:don’t understand want use pooled standard deviation display variability specific treatment?\nanswer : use type graph :\nanswer : use type graph :graph provides descriptive statistics - summary central tendancy variabilityHowever, address scientific questions, inferential statistics often must utilized determine different treatments relative background noise","code":"\nls1 <- lm(height ~type, data = darwin)\nestimates <- expand.grid(type = levels(darwin$type)) #distills a dataframe variable down to just its  unique levels \nestimates\n#>    type\n#> 1 Cross\n#> 2  Self\nestimates$height <- predict(ls1, newdata = estimates) #grabs the corresponding means calculated in ls1 \nestimates\n#>    type   height\n#> 1 Cross 20.19167\n#> 2  Self 17.57500\nlmDisplay <- display(lm(height ~type, data = darwin))\n#> lm(formula = height ~ type, data = darwin)\n#>             coef.est coef.se\n#> (Intercept) 20.19     0.76  \n#> typeSelf    -2.62     1.07  \n#> ---\n#> n = 30, k = 2\n#> residual sd = 2.94, R-Squared = 0.18\nlmDisplay\n#> $call\n#> lm(formula = height ~ type, data = darwin)\n#> \n#> $sigma.hat\n#> [1] 2.94038\n#> \n#> $r.squared\n#> [1] 0.175003\n#> \n#> $coef\n#> (Intercept)    typeSelf \n#>   20.191667   -2.616667 \n#> \n#> $se\n#> (Intercept)    typeSelf \n#>   0.7592028   1.0736749 \n#> \n#> $t.value\n#> (Intercept)    typeSelf \n#>   26.595880   -2.437113 \n#> \n#> $p.value\n#>  (Intercept)     typeSelf \n#> 2.049810e-21 2.141448e-02 \n#> \n#> $n\n#> [1] 30\n#> \n#> $k\n#> [1] 2\nestimates$SD <- rep(lmDisplay$sigma.hat, 2)\nestimates\n#>    type   height      SD\n#> 1 Cross 20.19167 2.94038\n#> 2  Self 17.57500 2.94038\nbase.plot10 <- ggplot(estimates, aes(x = type, y = height)) +\n    ylim(14, 25) + geom_point()\nbase.plot10\nFig10_1 <- \nbase.plot10 + \n    ggtitle(\"Mean and Standard Deviation\") + \n  geom_linerange(aes(ymin = height - SD, ymax = height + SD), size = 1)\nFig10_1"},{"path":"intervals.html","id":"inferential-statistics","chapter":"10 Intervals","heading":"10.4 Inferential statistics","text":"","code":""},{"path":"intervals.html","id":"standard-error-bars","chapter":"10 Intervals","heading":"10.4.1 Standard error bars","text":"Standard error can thought standard deviation estimated statisticsStandard error mean (SEM) SD estimated means\nReflects uncertainty related sampling\nDifferent samplings provide different means\n\nShows confidence true mean probably lies\nReflects uncertainty related sampling\nDifferent samplings provide different means\nDifferent samplings provide different meansShows confidence true mean probably liesSEM basis inferential statistics\nerror bars discussed multiples SEM\nerror bars discussed multiples SEMExtract standard error predict table:Draw graph means respective SEs:","code":"\nestimates$SEM <- predict(ls1, newdata = estimates, se.fit = TRUE)$se.fit\nestimates\n#>    type   height      SD       SEM\n#> 1 Cross 20.19167 2.94038 0.7592028\n#> 2  Self 17.57500 2.94038 0.7592028\nfig10_2a <- \n  ggplot(estimates, aes(x = type, y = height)) + ylim(14, 25) + geom_point() +\n ggtitle(\"A) Mean ± SEM\") + \n  geom_linerange(aes(ymin = height - SEM, ymax = height + SEM), size = 1)\nfig10_2a"},{"path":"intervals.html","id":"confidence-intervals-2","chapter":"10 Intervals","heading":"10.4.2 Confidence intervals","text":"95% CI make best general purpose interval recommended present casesThey show point estimates (means) plus/minus 2 standard errors\nsample sizes small, t-distribution increases number SEs required achieve level confidence\nsample sizes small, t-distribution increases number SEs required achieve level confidenceCIs express confidence estimate allow hypothesis testing :\nValues inside interval consistent data falling outside \nCIs can used test whether observed means significantly different hypothesized values (value, often zero - reject null hypothesis)\nCIs everything t- F-test \nValues inside interval consistent data falling outside notCIs can used test whether observed means significantly different hypothesized values (value, often zero - reject null hypothesis)CIs everything t- F-test moreExtract 95% CI values add estimates dataframe:Substitute CI values base plot:limitation show standard erros 95% CI bars show exacty two means statisticsally significant notShowing unambiguously can come two forms:\nPlot difference means difference’s corresponding CI\nPlot two means standard error bars show least significant difference (LSD)\nPlot difference means difference’s corresponding CIPlot two means standard error bars show least significant difference (LSD)","code":"\nestimates$CI95_lwr <- predict(ls1, newdata = estimates, interval = \"confidence\")[, 2] #the square bracket extracts the specific datapoint\nestimates$CI95_upr <- predict(ls1, newdata = estimates, interval = \"confidence\")[, 3]\nestimates\n#>    type   height      SD       SEM CI95_lwr CI95_upr\n#> 1 Cross 20.19167 2.94038 0.7592028 18.63651 21.74682\n#> 2  Self 17.57500 2.94038 0.7592028 16.01984 19.13016\nfig10_2b <- \nggplot(estimates, aes(x = type, y = height)) + ylim(14, 25) + geom_point() + #the base.plot10 wasn't working so I had to put the whole line of code in \n  ggtitle(\"B) Mean and 95% CI\") + \n  geom_linerange(aes(ymin = CI95_lwr, ymax = CI95_upr), size = 1) \nfig10_2b"},{"path":"intervals.html","id":"confidence-intervals-for-differences-between-means","chapter":"10 Intervals","heading":"10.4.3 Confidence intervals for differences between means","text":"Show whether difference means significant :\nFigure 10.1: Figure 10.3\nFigure 10.3, 0 outside 95% CI, can reject null hypothesis difference means level confidence\nissue method see means \nissue method see means themselvesWe can use LSD plot means using standard error difference construct confidence intervals show whether means significantly different ","code":"\nfig10_3 <- coefplot(ls1, xlim = c(-5,0))"},{"path":"intervals.html","id":"least-significant-differences","chapter":"10 Intervals","heading":"10.4.4 Least significant differences","text":"\\(LSD = t \\cdot SED\\)2 SEDs large sample sizeSo LSD equivalent 95% CI difference meansThis type intervals shows means shared interval \noverlap shared interval mean indicate two significantly different\noverlap shared interval mean indicate two significantly differentExtract SED linear model:Calculate t (critical) value:Center LSD interval mean (show estimate \\(\\pm\\) half LSD):\nFigure 10.2: Figure 10-2C\noverlap error bars Figure 10-2C indicate means significantly different \ncase, overlap; therefore, means different 95% CI\ncase, overlap; therefore, means different 95% CI","code":"\nestimates$SED <- summary(ls1)$coefficients[2,2] #the values in the brackets show that we want to extract the value in the 2nd row and 2nd column\nestimates\n#>    type   height      SD       SEM CI95_lwr CI95_upr\n#> 1 Cross 20.19167 2.94038 0.7592028 18.63651 21.74682\n#> 2  Self 17.57500 2.94038 0.7592028 16.01984 19.13016\n#>        SED\n#> 1 1.073675\n#> 2 1.073675\nqt(0.975, df = 28)\n#> [1] 2.048407\nfig10_2c <- \nggplot(estimates, aes(x = type, y = height)) + ylim(14, 25) + geom_point() + ggtitle(\"C) Mean and LSD\") + \n  geom_linerange(aes(ymin = height - 1.02 *SED, ymax= height + 1.02*SED), size = 1)\nfig10_2c\nfig10_2 <- (fig10_2a| fig10_2b|fig10_2c)\nfig10_2"},{"path":"intervals.html","id":"multi-interval-plots","chapter":"10 Intervals","heading":"10.4.5 Multi-interval plots","text":"Plot multiple intervals different levels significance(use different multiples SEM):","code":"\nFig10_4a <- \nggplot(estimates, aes(x = type, y = height)) + ylim (11, 27) +\ngeom_point(shape = 3) + \nggtitle(\"A) Multi-interval plot\") +\n  geom_linerange(aes(min = height - 1*SEM, max = height + 1*SEM), size = 1.4) + #67% CI\n  geom_linerange(aes(min = height - 2*SEM, max = height + 2*SEM), size = 0.7) + #95% CI\n  geom_linerange(aes(min = height - 3*SEM, max = height + 3*SEM), size = 0.3)   #99% CI\nFig10_4a"},{"path":"intervals.html","id":"prediction-intervals","chapter":"10 Intervals","heading":"10.4.6 Prediction intervals","text":"Plot graph prediction interval:Show prediction interval substantially wider:","code":"\nestimates$PI95_lwr <- predict(ls1, newdata = estimates, interval = \"prediction\")[, 2] #the square bracket extracts the specific datapoint\nestimates$PI95_upr <- predict(ls1, newdata = estimates, interval = \"prediction\")[, 3]\nestimates\n#>    type   height      SD       SEM CI95_lwr CI95_upr\n#> 1 Cross 20.19167 2.94038 0.7592028 18.63651 21.74682\n#> 2  Self 17.57500 2.94038 0.7592028 16.01984 19.13016\n#>        SED PI95_lwr PI95_upr\n#> 1 1.073675 13.97104 26.41229\n#> 2 1.073675 11.35437 23.79563\nfig10_4b <- \nggplot(estimates, aes(x = type, y = height)) + ylim(11, 27) + geom_point() + ggtitle(\"C) Mean and 95% PI\") + \n  geom_linerange(aes(ymin = PI95_lwr, ymax= PI95_upr), size = 1)\nfig10_4b\nfig10_4 <- (Fig10_4a|fig10_4b)\nfig10_4"},{"path":"intervals.html","id":"relating-different-types-of-interval-and-error-bar","chapter":"10 Intervals","heading":"10.5 Relating different types of interval and error bar","text":"SED basis relating differences meansIf know SED related SEM, can determine different intervals related eachother tooThe author presents rules thumb approximate eye ball tests looking means 95% CIs","code":""},{"path":"intervals.html","id":"interpreting-confidence-intervals","chapter":"10 Intervals","heading":"10.5.1 Interpreting confidence intervals","text":"author presents hypothetical scenario emphasize nuanced advantages estimation-based approachIn following example, three treatments - , B, C - generate different responses:Scenarios:\nGiven treatment , meaan close 0 95% CI encompasses 0\nsignificance test (t-test) produce non-significant result\n\nGiven treatment B, 95% CI width treatment , mean away 0 CI include \nsignificance test produce result showing statistical significance\n\nGiven treatment C, mean B 95% CI much wider encompasses zero\nsignificance test produce non-significant result\n\nGiven treatment , meaan close 0 95% CI encompasses 0\nsignificance test (t-test) produce non-significant result\nsignificance test (t-test) produce non-significant resultGiven treatment B, 95% CI width treatment , mean away 0 CI include \nsignificance test produce result showing statistical significance\nsignificance test produce result showing statistical significanceGiven treatment C, mean B 95% CI much wider encompasses zero\nsignificance test produce non-significant result\nsignificance test produce non-significant resultUsing traditional significance testing lead audience believe C treatments produce outcome B effectHowever, use estimates intervals encourages different thinkingThe results produced treatments B pretty straightforwardBut treatment C leads mean response similiar treatment B\njust accompanied greater uncertainty\nmeans larger response values plausible\njust accompanied greater uncertaintyBut means larger response values plausibleTreatment C’s effect non-significant convention p < 0.05But possibility large effect possibly even greater one produced treatment B\nthinking encourage consideration treatment C sample size\nstudy repeated examine effects treatment C perhaps larger sample size\nthinking encourage consideration treatment C sample sizeThe study repeated examine effects treatment C perhaps larger sample size","code":""},{"path":"intervals.html","id":"point-estimates-and-confidence-intervals-for-research-synthesis-and-meta-analysis","chapter":"10 Intervals","heading":"10.5.2 Point estimates and confidence intervals for research synthesis and meta-analysis","text":"Treatment C type ‘boring’ non-significant result unlikely publishedTherefore, less likely inclcuded meta-analysisMeta-analysis major tool detection repeatabilityIf meta analysis aiming examine effects inbreeding (selfing) maize fitness, might look Darwin’s maize data , purposes section, imaginary data WallaceDarwin’s data showed inbreeding passed lowest level confidence (95%)Whereas, Wallace showed statistically significant effectBut look studies using estimates intervals gives different impression:way looking data shows studies similiar terms level effectOne study’s interval just merely contains 0 (null hypothesis rejected) one just barely includes arbitrary null valueBoth studies together show consistency terms mean response variability (assuming wallace used similiar sample sizes) measuring effect inbreeding","code":""},{"path":"analysis-of-variance.html","id":"analysis-of-variance","chapter":"11 Analysis of Variance","heading":"11 Analysis of Variance","text":"","code":""},{"path":"analysis-of-variance.html","id":"anova-tables","chapter":"11 Analysis of Variance","heading":"11.1 ANOVA tables","text":"Pairwise t-tests become less effective experimental designs become complex\npairwise comparisons , greater risk generating false positives\npairwise comparisons , greater risk generating false positivesIt can useful instead use approach analysis varianceGenerally ANOVAs deal categorical explanatory variables, versions can evaluate continuous explanatory variables tooWe can use ANOVA evaluate Darwin’s maize data","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"SemiPar\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"SMPracticals\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(SemiPar)\nlibrary(SMPracticals)"},{"path":"analysis-of-variance.html","id":"anova-tables-darwins-maize","chapter":"11 Analysis of Variance","heading":"11.2 ANOVA tables: Darwin’s maize","text":"Simple linear model chapter 6:Since one explanatory variable, model called one-way ANOVAAn anova aims quantify overall variability within dataset, parse variability within groups (.e. self cross pollinated offspring)\ncalculate signal:noise ratio\ngreater signal noise ratio, confidence can whether test detected real effect\n\ncalculate signal:noise ratio\ngreater signal noise ratio, confidence can whether test detected real effect\ngreater signal noise ratio, confidence can whether test detected real effectThe variability quantified method known least squares\nmethod calculates overall variability (total sum squares, SST) measuring differences individual data points reference point, ‘intercept’:\nOften intuitive reference point use grand mean\nmethod calculates overall variability (total sum squares, SST) measuring differences individual data points reference point, ‘intercept’:Often intuitive reference point use grand mean\nFigure 11.1: Figure 11-1A. Total Sum Squares\nHorizontal line: grand (overall) meanVertical lines: measure differences data point reference (horizontal line)\ndifferences (distances reference line) squared summed\ndifferences (distances reference line) squared summedThe intuitive reference point grand mean, value can used\nStatistical software usually choose different reference point\nStatistical software usually choose different reference pointNext, least squares quantifies much overall variability explained classifying datapoints treatment groups:\nFigure 11.2: Figure 11-1B. Treatment Sum Squares\nfigure b\nnext step called least squares\nLeast squares quantifies much overall variability explained classifying datta points treatment groups\n\nvertical lines show differences overall mean (solid vertical line) treatment mean (row red crosses black circles (also fitted values))\nnext step called least squares\nLeast squares quantifies much overall variability explained classifying datta points treatment groups\nLeast squares quantifies much overall variability explained classifying datta points treatment groupsThe vertical lines show differences overall mean (solid vertical line) treatment mean (row red crosses black circles (also fitted values))Finally, residual variation explained:\n\n\nFigure 11.3: Figure 11-1C. Error sum squares\n\n\nFigure 11.3: Figure 11-1C. Error sum squares\nfigure c\ntreatment groups’ variability subtracted total variability, unexplained residual variability leftover\nvertical dotted lines show differences observed data values treatment level means\ntreatment groups’ variability subtracted total variability, unexplained residual variability leftoverThe vertical dotted lines show differences observed data values treatment level meansOverall, least squares method quantifies overall varability splits signal noise\nworks sums squared distances individual data points means - ‘sum squared differences’ ‘sum squares (SS)’\nworks sums squared distances individual data points means - ‘sum squared differences’ ‘sum squares (SS)’Draw analysis variance table model:output:\nfirst unnamed column - shows source variation (type Residuals)\nSources variation: pollination treatment (signal) residual variation (noise)\n\nDf (degrees freedom) - indication number treatment levels experimental units (sample size)\nfirst unnamed column - shows source variation (type Residuals)\nSources variation: pollination treatment (signal) residual variation (noise)\nSources variation: pollination treatment (signal) residual variation (noise)Df (degrees freedom) - indication number treatment levels experimental units (sample size)Sum Sq = sum squares\nR doesn’t provide total variation find just add two values SumSq \nMean Sq variance\nmean square calculated dividing row’s Sum Sq value corresponding Df value\nEssentially, gives average amount variability per treatment level per experimental unit\nmean square residual calculated pooling variation within samples order get better estimate overall noise\nuseful number different treatments smaller sample size\nLinear models almost always rely pooled estimate variance\n, standard errors estimates means differ treatment groups different sample sizes\n\nR doesn’t provide total variation find just add two values SumSq upMean Sq variance\nmean square calculated dividing row’s Sum Sq value corresponding Df value\nEssentially, gives average amount variability per treatment level per experimental unit\nmean square residual calculated pooling variation within samples order get better estimate overall noise\nuseful number different treatments smaller sample size\nLinear models almost always rely pooled estimate variance\n, standard errors estimates means differ treatment groups different sample sizes\nmean square calculated dividing row’s Sum Sq value corresponding Df valueEssentially, gives average amount variability per treatment level per experimental unitThe mean square residual calculated pooling variation within samples order get better estimate overall noiseThis useful number different treatments smaller sample sizeLinear models almost always rely pooled estimate varianceBecause , standard errors estimates means differ treatment groups different sample sizes","code":"\nls1 <- lm(height ~ type, data = darwin)\nanova(ls1)\n#> Analysis of Variance Table\n#> \n#> Response: height\n#>           Df  Sum Sq Mean Sq F value  Pr(>F)  \n#> type       1  51.352  51.352  5.9395 0.02141 *\n#> Residuals 28 242.083   8.646                  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"analysis-of-variance.html","id":"hypothesis-testing-f-values","chapter":"11 Analysis of Variance","heading":"11.3 Hypothesis testing: F-values","text":"signal noise ratio calculated dividing treatment variance (mean square) residual error variance (mean square) produce F-value given fifth column:anova table, signal estimated signal (51.352) 5.9x larger estimated noise (8.646)\nlarger signal noise ratio, confident can false positive\nlarger signal noise ratio, confident can false positivethe sixth column - p value; says 2% chance observing signal noise ratio least large null hypothesis true (.e. actually effect pollination type)\nrepeated Darwins experiment , expect, 2% time, actually effect pollination type expect F value size larger\nCritics argue P = 0.05 creates many false positives minimum P value < 0.004 appropriate\nGenerally, larger sample, lower p value (due increased statistical power)\nmakes hard use p values compare strength results different analyses sample sizes vary\np values generally create dichotomy significant non-significant result\n-Instead, estimates confidence intervals can used\nrepeated Darwins experiment , expect, 2% time, actually effect pollination type expect F value size largerCritics argue P = 0.05 creates many false positives minimum P value < 0.004 appropriateGenerally, larger sample, lower p value (due increased statistical power)makes hard use p values compare strength results different analyses sample sizes varyp values generally create dichotomy significant non-significant result\n-Instead, estimates confidence intervals can usedRecreate p value plugging values anova table:Often p values presented supporting information, readers can’t interpret without knowing test come number degrees freedomauthor’s suggested description analyses:\n\n\nFigure 11.4: Figure 11-1C. Error sum squares\n\n\nFigure 11.4: Figure 11-1C. Error sum squares\n","code":"\npf(\n  5.9395,  #the F value\n  1,       #degrees of freedom for the signal (numerator)\n  28,      #degrees of freedom for the noise (denominator)\n  lower.tail = FALSE) #prints the probability of being in the tail of the distribution with an F-value => the observed value (5.9) in this case approximately 2%\n#> [1] 0.02141466"},{"path":"analysis-of-variance.html","id":"two-way-anova","chapter":"11 Analysis of Variance","heading":"11.4 Two-way ANOVA","text":"two-way anova analysis deals two explanatory variablesIn example, continue use Darwin’s maize dataThe new explanatory variable use ‘plant pairing’Create linear model factors plant pairings:Compare tables one-way two-way anova:two way anova, find one “unsatisfying” thing darwin’s experiment: pairing doesn’t work way intendedIf worked, expected Mean Sq value larger Residuals (explained variability noise )\nPlant pairing explains 8.8% total variation\nPlant pairing explains 8.8% total variationSince mean sq pair (6.162) smaller mean sq residuals (11.130), implies ‘negative variance component’. two explanations :\nSampling variability\nissue experimental design\nDarwin didn’t randomize experimental units (experiment pre-dates principal)\n\nSampling variabilityAn issue experimental design\nDarwin didn’t randomize experimental units (experiment pre-dates principal)\nDarwin didn’t randomize experimental units (experiment pre-dates principal)","code":"\nls2 <- lm(height ~ type + pair, data = darwin)\noptions(show.signif.stars = FALSE) \nanova(ls1)\n#> Analysis of Variance Table\n#> \n#> Response: height\n#>           Df  Sum Sq Mean Sq F value  Pr(>F)\n#> type       1  51.352  51.352  5.9395 0.02141\n#> Residuals 28 242.083   8.646\nanova(ls2)\n#> Analysis of Variance Table\n#> \n#> Response: height\n#>           Df  Sum Sq Mean Sq F value Pr(>F)\n#> type       1  51.352  51.352  4.6139 0.0497\n#> pair      14  86.264   6.162  0.5536 0.8597\n#> Residuals 14 155.820  11.130"},{"path":"factorial-designs.html","id":"factorial-designs","chapter":"12 Factorial designs","heading":"12 Factorial designs","text":"","code":""},{"path":"factorial-designs.html","id":"introduction-6","chapter":"12 Factorial designs","heading":"12.1 Introduction","text":"experimental designs two categorical explanatory variablesCan look interaction effects variables","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"cowplot\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"patchwork\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"dplyr\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"readr\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(readr)"},{"path":"factorial-designs.html","id":"factorial-designs-1","chapter":"12 Factorial designs","heading":"12.2 Factorial designs","text":"Fully factorial designs analyze interactions applying two treatments (factors) possible combinationsThe effects treatments can analyzed treatment alone combination treatmentsThis example dataset chapter comes peer-reviewed publication (Hautier et al. 2009) response net aboveground plant biomass grassland plots evaluated treated fertilizer lightThe dataframe encompasses two variables (factors) factor two levels (.e. without fertilizer)\nfour possible treatment combinations\nfour possible treatment combinationsWe want determine interaction addition light fertilizer treatment together\nnegative interaction addition fertilizer creates canopy shades understory\npositive interaction synergistic effect grassland productivity terms biomass\nnegative interaction addition fertilizer creates canopy shades understoryIt positive interaction synergistic effect grassland productivity terms biomassLoad data create dataframe:dataset:\ncontrol (added fertilizer light: F-, L-)\nfertilizer (F+, L-)\nlight (F-, L+)\nfertilizer light added simultaneously (F+, L+)\nbiomass response variable\ncontrol (added fertilizer light: F-, L-)fertilizer (F+, L-)light (F-, L+)fertilizer light added simultaneously (F+, L+)biomass response variableThere two ways looking dataif focus column fl_, can approach dataset one-way anova design\nfl_ individual factor four levels\nfl_ individual factor four levelsif focus columns fert_ light_, two levels , approach design factorialGenerate summary statistics response variable biomass:","code":"\nurlfile=\"https://raw.githubusercontent.com/apicellap/data/main/Data_Factorial.txt\"\nfact<-read_table(url(urlfile))\n#> \n#> ── Column specification ────────────────────────────────────\n#> cols(\n#>   Fert = col_character(),\n#>   Light = col_character(),\n#>   FL = col_character(),\n#>   Biomass.m2 = col_double()\n#> )\nhead(fact)\n#> # A tibble: 6 × 4\n#>   Fert  Light FL    Biomass.m2\n#>   <chr> <chr> <chr>      <dbl>\n#> 1 F-    L-    F-L-        254.\n#> 2 F-    L-    F-L-        202 \n#> 3 F-    L-    F-L-        392.\n#> 4 F-    L-    F-L-        455.\n#> 5 F-    L-    F-L-        359.\n#> 6 F-    L-    F-L-        386.\nstr(fact)\n#> spec_tbl_df [64 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#>  $ Fert      : chr [1:64] \"F-\" \"F-\" \"F-\" \"F-\" ...\n#>  $ Light     : chr [1:64] \"L-\" \"L-\" \"L-\" \"L-\" ...\n#>  $ FL        : chr [1:64] \"F-L-\" \"F-L-\" \"F-L-\" \"F-L-\" ...\n#>  $ Biomass.m2: num [1:64] 254 202 392 455 359 ...\n#>  - attr(*, \"spec\")=\n#>   .. cols(\n#>   ..   Fert = col_character(),\n#>   ..   Light = col_character(),\n#>   ..   FL = col_character(),\n#>   ..   Biomass.m2 = col_double()\n#>   .. )\n names(fact)[names(fact) == 'Fert'] <- 'fert_'\n names(fact)[names(fact) == 'Light'] <- 'light_'\n names(fact)[names(fact) == 'FL'] <- 'fl_'\n names(fact)[names(fact) == 'Biomass.m2'] <- 'biomass'\n head(fact)\n#> # A tibble: 6 × 4\n#>   fert_ light_ fl_   biomass\n#>   <chr> <chr>  <chr>   <dbl>\n#> 1 F-    L-     F-L-     254.\n#> 2 F-    L-     F-L-     202 \n#> 3 F-    L-     F-L-     392.\n#> 4 F-    L-     F-L-     455.\n#> 5 F-    L-     F-L-     359.\n#> 6 F-    L-     F-L-     386.\nsummary(fact)\n#>     fert_              light_              fl_           \n#>  Length:64          Length:64          Length:64         \n#>  Class :character   Class :character   Class :character  \n#>  Mode  :character   Mode  :character   Mode  :character  \n#>                                                          \n#>                                                          \n#>                                                          \n#>     biomass     \n#>  Min.   :152.3  \n#>  1st Qu.:370.1  \n#>  Median :425.9  \n#>  Mean   :441.6  \n#>  3rd Qu.:517.2  \n#>  Max.   :750.4"},{"path":"factorial-designs.html","id":"comparing-three-or-more-groups","chapter":"12 Factorial designs","heading":"12.3 Comparing three or more groups","text":"chapter, build full factorial analysisFirst perform linear model analysis two factors interaction\ninvolve untreated control (F-L-) fertilizer (F+) light (L+) treatment\ninvolve untreated control (F-L-) fertilizer (F+) light (L+) treatmentSubset data look two invdividual treatments (encompass control):Generate summary statistics quick data visualization:Visualize subsetted data:Based Figure 12.1, might effect fertilizer relative control\ncan’t make guess effect light treatment based graph\ncan’t make guess effect light treatment based graphBut graph doesn’t tell us outliersQuick check outliers:Create linear model (one factor three levels)model treats design one-way anova using single factor\n(Intercept) actual value mean F-L-\nmean biomass \\(356 g\\) standard error \\(21 \\;g\\)\n\nfl_F-L+ difference mean factor intercept\ndifference (relative control) \\(30 g\\) standard error \\(30 \\;g\\)\n\nfl_F+L- difference mean factor intercept\ndifference (relative control) \\(94 g\\) standard error \\(30 \\;g\\)\nSignal seems outweigh noise difference 3x higher SED\n\nfertilizer treatment difference mean \\(60 \\;g\\) higher light treatment relative control\n\n(Intercept) actual value mean F-L-\nmean biomass \\(356 g\\) standard error \\(21 \\;g\\)\nmean biomass \\(356 g\\) standard error \\(21 \\;g\\)fl_F-L+ difference mean factor intercept\ndifference (relative control) \\(30 g\\) standard error \\(30 \\;g\\)\ndifference (relative control) \\(30 g\\) standard error \\(30 \\;g\\)fl_F+L- difference mean factor intercept\ndifference (relative control) \\(94 g\\) standard error \\(30 \\;g\\)\nSignal seems outweigh noise difference 3x higher SED\n\nfertilizer treatment difference mean \\(60 \\;g\\) higher light treatment relative control\ndifference (relative control) \\(94 g\\) standard error \\(30 \\;g\\)\nSignal seems outweigh noise difference 3x higher SED\nSignal seems outweigh noise difference 3x higher SEDThe fertilizer treatment difference mean \\(60 \\;g\\) higher light treatment relative controlVisualize treatment differences:plot shows fail reject null hypothesis light treatment\nconfidence light treatment produces real effect increasing biomass\nconfidence light treatment produces real effect increasing biomassHowever null hypothesis can rejected fertilizer treatment 95% CI","code":"\nsubs <- subset(fact, fl_ != \"F+L+\") \nhead(subs)\n#> # A tibble: 6 × 4\n#>   fert_ light_ fl_   biomass\n#>   <chr> <chr>  <chr>   <dbl>\n#> 1 F-    L-     F-L-     254.\n#> 2 F-    L-     F-L-     202 \n#> 3 F-    L-     F-L-     392.\n#> 4 F-    L-     F-L-     455.\n#> 5 F-    L-     F-L-     359.\n#> 6 F-    L-     F-L-     386.\ncntrl<- subset(subs, fl_ == \"F-L-\") \nlght <- subset(subs, fl_ == \"F-L+\")\nfert <- subset(subs, fl_ == \"F+L-\") \n\ncntrl.mean<- summarise(\n         group_by(cntrl, fl_), \n         biomass = mean(biomass)) \nlght.mean<- summarise(\n         group_by(lght, fl_), \n         biomass = mean(biomass)) \nfert.mean<- summarise(\n         group_by(fert, fl_), \n         biomass = mean(biomass))\nxlabels <- c(\"Control \\n (F-L-)\", \"Light only \\n (F-L+)\", \"Fertilizer only \\n (F+L-)\") #the \\n tells R to put the next bit of text on the next line \n\nggplot(subs, aes(x = fl_, y = biomass)) + \n  geom_point(data = subs, shape = 1, size = 3) + \n    geom_point(data = cntrl.mean, color = \"red\", size = 4) + \n    geom_point(data = lght.mean, color = \"red\", size = 4) +\n    geom_point(data = fert.mean, color = \"red\", size = 4) + \n  scale_x_discrete(limits = c(\"F-L-\", \"F-L+\", \"F+L-\"),\n                   labels = xlabels) + xlab(\"\")\nggplot(subs, aes(x = fl_, y = biomass)) + geom_boxplot() +  \n  scale_x_discrete(limits = c(\"F-L-\", \"F-L+\", \"F+L-\"),\n                   labels = xlabels) + xlab(\"\")\nmod1 <- lm(biomass ~ fl_, data = subs)\ndisplay(mod1)\n#> lm(formula = biomass ~ fl_, data = subs)\n#>             coef.est coef.se\n#> (Intercept) 355.79    21.41 \n#> fl_F-L+      30.12    30.27 \n#> fl_F+L-      93.69    30.27 \n#> ---\n#> n = 48, k = 3\n#> residual sd = 85.63, R-Squared = 0.18\nfig12_2 <- coefplot(mod1)"},{"path":"factorial-designs.html","id":"two-way-anova-no-interaction","chapter":"12 Factorial designs","heading":"12.4 Two-way ANOVA (no interaction)","text":"next step towards full factorial analysis, can fit model two factors (light_ fert_)allow interaction analysisCreate model two factors (two way anova without interaction):output exactly last model, one way anova fl_ factor 4 levelsHere though, fertilizer light treatment variables separately compared controlThere third comparison made one light fertilizer treatments\npossible pairwise comparisons can made degrees freedom (three treatment levels gives us two degrees freedom (n-1 = df))\npossible pairwise comparisons can made degrees freedom (three treatment levels gives us two degrees freedom (n-1 = df))R picked control (fl_F-L-) reference/intercept. use relevel function restructureIf specify priori comparisons ask major questions staying within limits degrees freedom disposal making adjustments multiple comparisons can avoided (avoid tend generate false positives)","code":"\ndisplay(lm(biomass ~ light_ + fert_, data = subs))\n#> lm(formula = biomass ~ light_ + fert_, data = subs)\n#>             coef.est coef.se\n#> (Intercept) 355.79    21.41 \n#> light_L+     30.12    30.27 \n#> fert_F+      93.69    30.27 \n#> ---\n#> n = 48, k = 3\n#> residual sd = 85.63, R-Squared = 0.18"},{"path":"factorial-designs.html","id":"additive-treatment-effects","chapter":"12 Factorial designs","heading":"12.5 Additive treatment effects","text":"able measure interactive effects, need able compare effect combined treatment (.e fertilizer plus light treatment) expect interaction\nfactorial anova creates -interaction scenario assuming treatment effects independent therefore additive\none treatment effect size \nanother treatment effect size B\nlinear model anova testing combination predict result + B\nterms data mean:\n\nfactorial anova creates -interaction scenario assuming treatment effects independent therefore additive\none treatment effect size \nanother treatment effect size B\nlinear model anova testing combination predict result + B\nterms data mean:\none treatment effect size Aand another treatment effect size Bthen linear model anova testing combination predict result + Bin terms data mean:123.8 g represents combined, additive expected effect two treatments (Light alone fertilizer alone)\ndifference\ncontrol coefficient 100 g, combined F+L+ treatment produce biomass 223.8 g\ndifferenceSo control coefficient 100 g, combined F+L+ treatment produce biomass 223.8 gThis additive prediction - threshold must passed us determine interaction effect fertilizer light treatments combinedPlot whole dataset:mean biomass F+L+ well threshold (blue line), probably positive interaction\nmeasure confidence assertion, create linear model entire dataset\nhelp us determine difference observed mean expected value relative background noise\nmeasure confidence assertion, create linear model entire datasetThis help us determine difference observed mean expected value relative background noiseCreate one-way ANOVA using single factor four levels:Author says first using one-way anova evaluate additive effect understand shortcomings testCompared mod1, fourth treatment combination changes pooled estimate variance (residual error - “residual sd”) therefore standard errors change . mod1, residual sd increased 6.93 unitscoefficients\nIntercept: displays mean biomass untreated control treatment standard error mean (\\(355.79 \\pm 23.14 \\;g\\))\nF-L+: displays difference (means) untreated control light treatment (\\(30.12 \\pm 32.72 \\;g\\))\nF+L-: displays difference (means) untreated control fertilizer treatment (\\(93.69 \\pm 32.72 \\;g\\))\nF+L+: displays difference (means) untreated control combined treatment (\\(219.23 \\pm 32.72 \\;g\\))\nIntercept: displays mean biomass untreated control treatment standard error mean (\\(355.79 \\pm 23.14 \\;g\\))F-L+: displays difference (means) untreated control light treatment (\\(30.12 \\pm 32.72 \\;g\\))F+L-: displays difference (means) untreated control fertilizer treatment (\\(93.69 \\pm 32.72 \\;g\\))F+L+: displays difference (means) untreated control combined treatment (\\(219.23 \\pm 32.72 \\;g\\))Visualize coefficients plotting :Figure 12.4 takeaways:\ndiscernible effect adding light\naddition fertilizer alone effect can confident lower bound well positive territory\nreasons, can confident combination treatment effect\nconfidence intervals provide us range plausible effect sizes consistent data (something using p values )\ndiscernible effect adding lightThe addition fertilizer alone effect can confident lower bound well positive territoryFor reasons, can confident combination treatment effectThe confidence intervals provide us range plausible effect sizes consistent data (something using p values )Adding light fertilizer increase biomass 30 94 g, respectively\ntreatments acted independently effects additive, expect increase relative control 124 g\nturns adding treatments combination increases biomass approximately 220 g\nsubstantially mean effects found either treatment applied alone\n\ntreatments acted independently effects additive, expect increase relative control 124 gIt turns adding treatments combination increases biomass approximately 220 g\nsubstantially mean effects found either treatment applied alone\nsubstantially mean effects found either treatment applied aloneThe 220 g increase almost 100 g higher additive prediction 124 gWe estimate size interaction\ncan accomplished manually standard error one way ANOVA\ncan accomplished manually standard error one way ANOVATherefore need linear model takes account factorial design","code":"\ncoef(mod1)[2] + #the coefficient of fl_F-L+   (30 g)          - the difference between the mean of F-L+ and the mean of the intercept \n  coef(mod1)[3] #the coefficient of fl_F+L-   (94 g)          - the difference between the mean of F+L- and the mean of the intercept \n#>  fl_F-L+ \n#> 123.8187\ncoef(mod1)[1] + #the coefficient of Intercept (fl_F-L-) - the mean of the intercept \ncoef(mod1)[2] + #the coefficient of fl_F-L+             - the difference between the mean of F-L+ and the mean of the intercept \n  coef(mod1)[3] #the coefficient of fl_F+L-             - the difference between the mean of F+L- and the mean of the intercept \n#> (Intercept) \n#>    479.6125\nxlabels2 <- c(\"Control \\n (F-L-)\", \"Light only \\n (F-L+)\", \"Fertilizer only \\n (F+L-)\", \"Fertilizer & Light \\n (F+L+)\") \nylabel <- expression(paste(\"Aboveground biomass (g m\"^\"-2\",\")\"))\nfig12_3 <- \n  ggplot(fact, aes(x = fl_, y = biomass)) + \n  geom_point(data = fact, shape = 1, size = 3) +\n  scale_x_discrete(limits = c(\"F-L-\", \"F-L+\", \"F+L-\", \"F+L+\"),\n                   labels = xlabels2) + xlab(\"\") + ylab(ylabel) +\n  scale_y_continuous(\n    limits = c(0,800),\n    breaks = seq(0,800, by = 50)\n  ) +\n  geom_hline(yintercept = 479.6125, color = \"blue\") + #add horizontal line at this y position - represents the additive prediction \n  stat_summary(fun = mean, geom = \"point\", color = \"red\", shape = 18, size = 4) + #calculate statistics (in this case the only mean) across all four levels of the categorical variable, fl_\n  theme_bw()\nfig12_3\nmod2 <- lm(biomass ~ fl_, data = fact)\ndisplay(mod2)\n#> lm(formula = biomass ~ fl_, data = fact)\n#>             coef.est coef.se\n#> (Intercept) 355.79    23.14 \n#> fl_F-L+      30.12    32.72 \n#> fl_F+L-      93.69    32.72 \n#> fl_F+L+     219.22    32.72 \n#> ---\n#> n = 64, k = 4\n#> residual sd = 92.56, R-Squared = 0.47\ncoefplot(mod2)"},{"path":"factorial-designs.html","id":"interactions-factorial-anova","chapter":"12 Factorial designs","heading":"12.6 Interactions: Factorial ANOVA","text":"factorial ANOVA allows estimation interactive effect directly produce standard error coefficient tooThe standard error can used calculate measure confidence can formally test interaction effectVisualize interaction:","code":"\nfig12_5a <- with(data = fact, \n                 interaction.plot(light_, #display on x axis\n                                  fert_,  #display for lines\n                                  biomass)) #the response variable \nfig12_5b <- with(data = fact, \n                 interaction.plot(fert_, light_, biomass))"},{"path":"factorial-designs.html","id":"factorial-anova-in-r","chapter":"12 Factorial designs","heading":"12.6.1 Factorial ANOVA in R","text":"factorial anova allows direct estimation interactive effect standard error\nstandard error can used measure confidence around asserted effect\nstandard error can used measure confidence around asserted effectVisualize interaction data:","code":"\nfig12_5a <- \n  with(        #interaction.plot doesn't have an argument that lets you provide the dataframe so you have to do it using the with() function \n    data = fact, \n    interaction.plot(\n      fert_,   #variable on the x axis \n      light_,  #variable inscribed into the lines \n      biomass, #response (y) variable \n      xlab = \"Fertilizer\"\n    )\n)\nfig12_5b <- \n  with(        #interaction.plot doesn't have an argument that lets you provide the dataframe so you have to do it using the with() function \n    data = fact, \n    interaction.plot(\n      light_,   #variable on the x axis \n      fert_,  #variable inscribed into the lines \n      biomass, #response (y) variable \n      xlab = \"Light\"\n    )\n)"},{"path":"factorial-designs.html","id":"factorial-anova-in-r-1","chapter":"12 Factorial designs","heading":"12.6.2 Factorial ANOVA in R","text":"Create model takes account interaction:Table coefficientsThe factorial analysis less power estimate interaction\nevidenced fact light_L+:fert_F+ row larger SE variables\nevidenced fact light_L+:fert_F+ row larger SE variablesThe interaction coef.se larger standard errors main effects\ntest less estimating power interaction\ntest less estimating power interactionIt’s important note coefficient last line 95.41 difference mean mean control’s biomass\ncombined treatment actually generated biomass equal additive prediction (see blue line fig12_3), :\nbiomass control + biomass increase due fertilizer + biomass increcase due light, \ncoefficient light_L+:fert_F+ zero since difference mean control mean combined treatments\n\ncombined treatment actually generated biomass equal additive prediction (see blue line fig12_3), :\nbiomass control + biomass increase due fertilizer + biomass increcase due light, \ncoefficient light_L+:fert_F+ zero since difference mean control mean combined treatments\nbiomass control + biomass increase due fertilizer + biomass increcase due light, thenThen coefficient light_L+:fert_F+ zero since difference mean control mean combined treatmentsVisualize coefficients table:Calculate mean biomass light treatment:Calculate mean biomass fertilizer treatment:Calculate mean biomass interaction treatment:table coefficients, mean given intercept (case, control) coefficient corresponding standard error - standard error mean (SEM)next two rows - light_L+ fert_F+ - also coefficients\ncoefficients differences means (respective variable control)\ncoefficient’s standard errors standard error difference (SED)\ncoefficients differences means (respective variable control)coefficient’s standard errors standard error difference (SED)third row, contains interaction, also gives information interaction’s difference mean prediction mean control plus corresponding SEDGenerate confidence intervals estimates:Based CIs, author suggests caution make mistake deciding significant effect light since interval contains zeroHe goes say evidence interaction effect data two treatments, treatments must investigated thoroughly contributionAll treatments must investigated even treatment’s main effect non-significantCreate model (two way anova) doesn’t evaluate interaction:table, R renames models cited anova()function compares two models using f-testThe table involves:\nidentity model (1 2)\nnumber degrees freedom\nresidual sum squares\nDf - shows 1 degree freedom used\nSum squares\nProbability value - shows null hypothesis can rejected (just barely beneath 0.05 threshold)\nAuthor goes say later mistake focus solely p-value since effect size large\nreferring fact combined treatment fertilizer + light led substantially biomass expected treatments added isolation\n\n\nidentity model (1 2)number degrees freedomThe residual sum squaresDf - shows 1 degree freedom usedSum squaresProbability value - shows null hypothesis can rejected (just barely beneath 0.05 threshold)\nAuthor goes say later mistake focus solely p-value since effect size large\nreferring fact combined treatment fertilizer + light led substantially biomass expected treatments added isolation\n\nAuthor goes say later mistake focus solely p-value since effect size large\nreferring fact combined treatment fertilizer + light led substantially biomass expected treatments added isolation\nreferring fact combined treatment fertilizer + light led substantially biomass expected treatments added isolationThe drop1() function twist:twist drop1() starts highest order (case, ’s interaction)highest order significant analysis stops \nAuthors says stops analysis rather “inappropriately” continuing look main effects presence interaction\nAuthors says stops analysis rather “inappropriately” continuing look main effects presence interactionThe drop1() function also provides value Akaike information criterion (AIC) can used model selection alternative P-values - laterThe author goes say p-values going provided, include information analysis performed, value test statistic, exact value P, number degrees freedom. can done like :Since interaction effect robust result, might misleading say main effect light non-significant instead main effects individual treatments reported","code":"\nmod3 <- lm(biomass ~ light_+ fert_ + fert_:light_, data = fact)\ndisplay(mod3)\n#> lm(formula = biomass ~ light_ + fert_ + fert_:light_, data = fact)\n#>                  coef.est coef.se\n#> (Intercept)      355.79    23.14 \n#> light_L+          30.13    32.72 \n#> fert_F+           93.69    32.72 \n#> light_L+:fert_F+  95.41    46.28 \n#> ---\n#> n = 64, k = 4\n#> residual sd = 92.56, R-Squared = 0.47\nfig12_6 <- coefplot(mod3)\ncoef(mod3)[1] + #coefficient of the intercept (this is the mean for the control)\n  coef(mod3)[2] #coefficient for light_L+ (this is the difference between the biomass means of the control and light)\n#> (Intercept) \n#>    385.9187\ncoef(mod3)[1] + #coefficient of the intercept (this is the mean for the control)\n  coef(mod3)[3] #coefficient for fert_F+ (this is the difference between the biomass means of the control and fertilizer treatment)\n#> (Intercept) \n#>    449.4875\ncoef(mod3)[1] + \n  coef(mod3)[2] +\n  coef(mod3)[3] + \n  coef(mod3)[4] #coefficient for light_L+:fert_F+ (this is the difference between the biomass means of the control and combination treatment) \n#> (Intercept) \n#>    575.0187\nconfint(mod3)\n#>                       2.5 %    97.5 %\n#> (Intercept)      309.508816 402.07868\n#> light_L+         -35.331781  95.58178\n#> fert_F+           28.236969 159.15053\n#> light_L+:fert_F+   2.836382 187.97612\nmod4 <- lm(biomass ~ light_ + fert_, data = fact)\noptions(show.signif.stars = FALSE)\nanova(mod3, mod4)\n#> Analysis of Variance Table\n#> \n#> Model 1: biomass ~ light_ + fert_ + fert_:light_\n#> Model 2: biomass ~ light_ + fert_\n#>   Res.Df    RSS Df Sum of Sq      F  Pr(>F)\n#> 1     60 513998                            \n#> 2     61 550407 -1    -36409 4.2501 0.04359\ndrop1(mod3, test = \"F\")\n#> Single term deletions\n#> \n#> Model:\n#> biomass ~ light_ + fert_ + fert_:light_\n#>              Df Sum of Sq    RSS    AIC F value  Pr(>F)\n#> <none>                    513998 583.43                \n#> light_:fert_  1     36409 550407 585.81  4.2501 0.04359"},{"path":"analysis-of-covariance.html","id":"analysis-of-covariance","chapter":"13 Analysis of Covariance","heading":"13 Analysis of Covariance","text":"","code":""},{"path":"analysis-of-covariance.html","id":"introduction-7","chapter":"13 Analysis of Covariance","heading":"13.1 Introduction","text":"last analysis dealt interaction two categorical explanatory variablesUsing ANCOVA, interactions categorical continuous explanatory variables can examinedSide note: ‘ANCOVA’ also used describe analysis need adjust effects uncontrolled variables called covariates","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"cowplot\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"patchwork\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"dplyr\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"Sleuth3\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(Sleuth3)"},{"path":"analysis-of-covariance.html","id":"the-agricultural-pollution-data","chapter":"13 Analysis of Covariance","heading":"13.2 The agricultural pollution data","text":"section ANCOVA, look subset variables study effects low-level atmospheric pollutants drought agricultural yieldsThe aim experiment determine yields two types soya beans called ‘Forrest’ ‘William’ affected two pollutants (low-level ozone \\(O_3\\) sulfur dioxide \\(SO_2\\)) also pollutants interact water stressWe examine effects ‘William’ varietyWe look effect pollutants water stress separate ANCOVAs combining three variables one linear modelQuartile values:Structure data:Stress factor 2 levelsSO2 O3, treatments, also levels even though read dataframe numeric values:Levels \\(SO_2\\) \\(O_3\\):coerced factors (using .factor()), maintain status numeric values form continuum\nallows examination continuous variables\nallows examination continuous variablesThis dataset balanced 2 x 3 x 5 fully factorial design (respect water stress x \\(SO_2\\) x \\(O_3\\))\n‘balanced’ meant equal number replicates treatment combination\ncompletely randomized design treatment combinations randomly positions one 30 growth chambers\n‘balanced’ meant equal number replicates treatment combinationIt completely randomized design treatment combinations randomly positions one 30 growth chambers","code":"\nhead(case1402)\n#>         Stress  SO2   O3 Forrest William\n#> 1 Well-watered 0.00 0.02    4376    5561\n#> 2 Well-watered 0.00 0.05    4544    5947\n#> 3 Well-watered 0.00 0.07    2806    4273\n#> 4 Well-watered 0.00 0.08    3339    3470\n#> 5 Well-watered 0.00 0.10    3320    3080\n#> 6 Well-watered 0.02 0.02    3747    5092\nsummary(case1402)\n#>           Stress        SO2                O3       \n#>  Stressed    :15   Min.   :0.00000   Min.   :0.020  \n#>  Well-watered:15   1st Qu.:0.00000   1st Qu.:0.050  \n#>                    Median :0.02000   Median :0.070  \n#>                    Mean   :0.02667   Mean   :0.064  \n#>                    3rd Qu.:0.06000   3rd Qu.:0.080  \n#>                    Max.   :0.06000   Max.   :0.100  \n#>     Forrest        William    \n#>  Min.   :2158   Min.   :2101  \n#>  1st Qu.:3245   1st Qu.:2889  \n#>  Median :3478   Median :3428  \n#>  Mean   :3699   Mean   :3635  \n#>  3rd Qu.:4327   3rd Qu.:4263  \n#>  Max.   :5573   Max.   :5947\nstr(case1402)\n#> 'data.frame':    30 obs. of  5 variables:\n#>  $ Stress : Factor w/ 2 levels \"Stressed\",\"Well-watered\": 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ SO2    : num  0 0 0 0 0 0.02 0.02 0.02 0.02 0.02 ...\n#>  $ O3     : num  0.02 0.05 0.07 0.08 0.1 0.02 0.05 0.07 0.08 0.1 ...\n#>  $ Forrest: int  4376 4544 2806 3339 3320 3747 4570 4635 3613 3259 ...\n#>  $ William: int  5561 5947 4273 3470 3080 5092 4752 4232 2867 3106 ...\nunique(case1402$SO2)\n#> [1] 0.00 0.02 0.06\nunique(case1402$O3)\n#> [1] 0.02 0.05 0.07 0.08 0.10"},{"path":"analysis-of-covariance.html","id":"ancova-with-water-stress-and-low-level-ozone","chapter":"13 Analysis of Covariance","heading":"13.3 ANCOVA with water stress and low-level ozone","text":"subsection focus \\(0_3\\) water stress perform ANCOVA impacts yield ‘William’ beansIn type experiment soya yields log transformed, \nHowever, must keep transformation mind later interpretting results\nHowever, must keep transformation mind later interpretting resultsPossible outcomes:\nstresses decrease yields\nOne plant exposed drought stress might susceptible pollutant stress (vice versa) - syngergistic effect\nObviously negative yields, yields bounded 0 - impose constraints\nstresses decrease yieldsOne plant exposed drought stress might susceptible pollutant stress (vice versa) - syngergistic effectObviously negative yields, yields bounded 0 - impose constraintsPretty labels:Create base plot data:Visualize data two regressions:figure, ANCOVA produces two regressions\none regression level water stress factor\ngeom_smooth() adds slopes\none regression level water stress factorgeom_smooth() adds slopesBased figure,\nthink probably effect ozone soya bean yield\nalso effect drought stress y intercepts lines look fairly similiar\nanything, guess effects additive interaction\n\nauthor also notes regression line stressed treatment slightly flatter well-watered one\nthink probably effect ozone soya bean yieldThere also effect drought stress y intercepts lines look fairly similiar\nanything, guess effects additive interaction\nanything, guess effects additive interactionThe author also notes regression line stressed treatment slightly flatter well-watered oneCreate model considers possibility interactionThis model similar two-way factorial ANOVA previous chapter. difference one explanatory variables continousThe author skips checking assumptions\nsays perfectly normally distributed/equal variances, major issues\nsays perfectly normally distributed/equal variances, major issuesThis ANCOVA can thought regressions within factor levels\nTherefore table coefficients contains four values:\nTwo regression intercepts two regression slopes\nOne intercept slope value regression\n\nTherefore table coefficients contains four values:\nTwo regression intercepts two regression slopes\nOne intercept slope value regression\nTwo regression intercepts two regression slopesOne intercept slope value regressionThe linear model prints one factor level “(Intercept)”\nprovides intercept slope one regression lines first two rows\nprovides difference intercept slope relative factor one regression lines next two rows\nprovides intercept slope one regression lines first two rowsThen provides difference intercept slope relative factor one regression lines next two rowsLabels (bottom top):\nO3:StressWell-watered - line addresses interaction\nProvides difference slopes two stress levels\n\nStressWell-watered - line indicates effect well watered treatment\nProvides difference intercepts regressions two stress levels\n\n03 - ozone\nIntercept - stressed level stress factor\nO3:StressWell-watered - line addresses interaction\nProvides difference slopes two stress levels\nProvides difference slopes two stress levelsStressWell-watered - line indicates effect well watered treatment\nProvides difference intercepts regressions two stress levels\nProvides difference intercepts regressions two stress levels03 - ozoneIntercept - stressed level stress factorUsing arithmetic, can solve four values two intercepts slopesThe coefficent difference slopes fourth row, interactive effect, -1.35 standard error -1.98\nestimated difference smaller SE, conventional rule thumb suggests support idea interactive effect\nestimated difference smaller SE, conventional rule thumb suggests support idea interactive effectTo precise, can look confidence intervals see row interactive effect contains zero:Use anova() function formally test interaction:interactive effect row small F value p value 0.50The drop1() function can also used first assess interaction:","code":"\nxlabel <- expression(paste(\"Ozone (µL • L\" ^ \"-1\", \")\"))\nylabel <- expression(paste(\"Log Yield (kg • ha\" ^ \"-1\", \")\")) \nfig13_0 <- ggplot(case1402, aes(x= O3, \n                                y = log(William))) + #log transform the y values \n  geom_point() + \n  xlab(xlabel) + ylab(ylabel) + \nscale_x_continuous(limits = c(0, 0.1), #set x axis range \n                    breaks = seq(0,0.1, by = 0.025)) +  #x axis range and increments \n  facet_wrap(vars(Stress)) + theme_bw()\nfig13_0\nfig13_1 <- fig13_0 + geom_smooth(method = \"lm\") + \nggtitle(\"Yield of 'William' soya beans under diverse stress conditions\") \nfig13_1\n#> `geom_smooth()` using formula 'y ~ x'\nw1 <- lm(log(William) ~ O3 * Stress, data = case1402)\ndisplay(w1)\n#> lm(formula = log(William) ~ O3 * Stress, data = case1402)\n#>                       coef.est coef.se\n#> (Intercept)            8.49     0.10  \n#> O3                    -6.47     1.40  \n#> StressWell-watered     0.26     0.14  \n#> O3:StressWell-watered -1.35     1.98  \n#> ---\n#> n = 30, k = 4\n#> residual sd = 0.15, R-Squared = 0.71\n#find the y intercept of the well-watered condition regression line \ncoef(w1)[1] + #coefficient of the stressed condition \n  coef(w1)[3] #coefficient of the difference for the well watered condition \n#> (Intercept) \n#>     8.75446\n\n#Find the slope of the well-watered condition regression line \ncoef(w1)[2] +\n  coef(w1)[4] #coefficient for the difference in slope of the well-watered condition \n#>       O3 \n#> -7.81444\nconfint(w1) #the fourth row encompasses 0\n#>                             2.5 %     97.5 %\n#> (Intercept)            8.28982454  8.6906347\n#> O3                    -9.34777877 -3.5865375\n#> StressWell-watered    -0.01918491  0.5476463\n#> O3:StressWell-watered -5.42109500  2.7265306\ncoefplot(w1)\nanova(w1)\n#> Analysis of Variance Table\n#> \n#> Response: log(William)\n#>           Df  Sum Sq Mean Sq F value    Pr(>F)    \n#> O3         1 1.13812 1.13812 51.9277 1.185e-07 ***\n#> Stress     1 0.23764 0.23764 10.8426  0.002859 ** \n#> O3:Stress  1 0.01013 0.01013  0.4621  0.502639    \n#> Residuals 26 0.56985 0.02192                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ndrop1(w1, test = \"F\")\n#> Single term deletions\n#> \n#> Model:\n#> log(William) ~ O3 * Stress\n#>           Df Sum of Sq     RSS     AIC F value Pr(>F)\n#> <none>                 0.56985 -110.91               \n#> O3:Stress  1  0.010129 0.57998 -112.38  0.4621 0.5026"},{"path":"analysis-of-covariance.html","id":"interactions-in-ancova","chapter":"13 Analysis of Covariance","heading":"13.4 Interactions in ANCOVA","text":"Now established interaction, can one two things:\nKeep model move interpret main effects ozone stress\ntake model simplification approach remove unimportant variables attain ‘minimal adequate model’ terms supports\nKeep model move interpret main effects ozone stressOr take model simplification approach remove unimportant variables attain ‘minimal adequate model’ terms supportsThere definitive answer, author moves model simplificationThis produces model terms main effects drought stress ozoneNow questions :\nrelationship (slope) ozone?\ncommon relationship need two lines different slopes two water stress treatments?\nrelationship (slope) ozone?common relationship need two lines different slopes two water stress treatments?cursory look ozone (\\(-7.14 \\pm 0.98\\)) difference intercepts (\\(0.18 \\pm 0.05\\)) shows can confident effectsNeither CI either ozone stress factor contains 0:can reject null hypothesis effect yield","code":"\nw2 <- lm(log(William) ~ O3 + Stress, data = case1402)\ndisplay(w2)\n#> lm(formula = log(William) ~ O3 + Stress, data = case1402)\n#>                    coef.est coef.se\n#> (Intercept)         8.53     0.07  \n#> O3                 -7.14     0.98  \n#> StressWell-watered  0.18     0.05  \n#> ---\n#> n = 30, k = 3\n#> residual sd = 0.15, R-Squared = 0.70\nconfint(w2)\n#>                          2.5 %     97.5 %\n#> (Intercept)         8.38292735  8.6837580\n#> O3                 -9.15368606 -5.1279124\n#> StressWell-watered  0.06819616  0.2878131\ncoefplot(w2)"},{"path":"analysis-of-covariance.html","id":"general-linear-models-three-way","chapter":"13 Analysis of Covariance","heading":"13.5 General linear models (Three-way)","text":"author ends chapter positing can take ANCOVA analysis step creating GLM analysis water stress, ozone, sulfur dioxide combinationThis design allows us include look three way interaction, might want unless priori question motivates inclusion third termWith complex designs involve terms, results can uninterpretableIn situation, don’t priori hypothesis reasonable three stresses interact reduce yield synergisticallyThis ANOVA table provides even greater support interactions, tests dual interactions tooSimplify model eliminate analysis interactions:table coefficients shows S)2 coefficient estimate -4.7x larger standard errorLook F p values anova table SO2 term:ANCOVAs three-way general linear model support clear negative effects water stress, ozone, sulfure dioxide soya bean yield ‘William’ variety","code":"\nsummary(lm(log(William) ~ O3 * SO2 * Stress, data = case1402))\n#> \n#> Call:\n#> lm(formula = log(William) ~ O3 * SO2 * Stress, data = case1402)\n#> \n#> Residuals:\n#>       Min        1Q    Median        3Q       Max \n#> -0.222274 -0.067130  0.007202  0.054343  0.213667 \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value\n#> (Intercept)                 8.5184     0.1111  76.703\n#> O3                         -5.4844     1.5963  -3.436\n#> SO2                        -1.0567     3.0414  -0.347\n#> StressWell-watered          0.3605     0.1571   2.295\n#> O3:SO2                    -36.8530    43.7175  -0.843\n#> O3:StressWell-watered      -2.5543     2.2576  -1.131\n#> SO2:StressWell-watered     -3.6102     4.3012  -0.839\n#> O3:SO2:StressWell-watered  45.2629    61.8259   0.732\n#>                           Pr(>|t|)    \n#> (Intercept)                < 2e-16 ***\n#> O3                         0.00236 ** \n#> SO2                        0.73158    \n#> StressWell-watered         0.03162 *  \n#> O3:SO2                     0.40831    \n#> O3:StressWell-watered      0.27005    \n#> SO2:StressWell-watered     0.41030    \n#> O3:SO2:StressWell-watered  0.47183    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.1152 on 22 degrees of freedom\n#> Multiple R-squared:  0.8507, Adjusted R-squared:  0.8032 \n#> F-statistic: 17.91 on 7 and 22 DF,  p-value: 9.896e-08\nconfint(lm(log(William) ~ O3 * SO2 * Stress, data = case1402))\n#>                                   2.5 %      97.5 %\n#> (Intercept)                  8.28808808   8.7487263\n#> O3                          -8.79501554  -2.1738073\n#> SO2                         -7.36420566   5.2508903\n#> StressWell-watered           0.03478276   0.6862235\n#> O3:SO2                    -127.51762948  53.8116264\n#> O3:StressWell-watered       -7.23619363   2.1276089\n#> SO2:StressWell-watered     -12.53043691   5.3100030\n#> O3:SO2:StressWell-watered  -82.95626522 173.4820277\ncoefplot(lm(log(William) ~ O3 * SO2 * Stress, data = case1402))\ndisplay(lm(log(William) ~ O3 + SO2 + Stress, data = case1402))\n#> lm(formula = log(William) ~ O3 + SO2 + Stress, data = case1402)\n#>                    coef.est coef.se\n#> (Intercept)         8.63     0.06  \n#> O3                 -7.14     0.74  \n#> SO2                -3.77     0.80  \n#> StressWell-watered  0.18     0.04  \n#> ---\n#> n = 30, k = 4\n#> residual sd = 0.11, R-Squared = 0.84\ncoefplot(lm(log(William) ~ O3 + SO2 + Stress, data = case1402))\nanova(lm(log(William) ~ O3 + SO2 + Stress, data = case1402))\n#> Analysis of Variance Table\n#> \n#> Response: log(William)\n#>           Df  Sum Sq Mean Sq F value    Pr(>F)    \n#> O3         1 1.13812 1.13812  94.119 3.969e-10 ***\n#> SO2        1 0.26558 0.26558  21.963 7.690e-05 ***\n#> Stress     1 0.23764 0.23764  19.652 0.0001501 ***\n#> Residuals 26 0.31440 0.01209                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-model-complexities.html","id":"linear-model-complexities","chapter":"14 Linear Model Complexities","heading":"14 Linear Model Complexities","text":"","code":""},{"path":"linear-model-complexities.html","id":"introduction-8","chapter":"14 Linear Model Complexities","heading":"14.1 Introduction","text":"Earlier chapters investigated complex forms ANOVA plus ANCOVA tooThere complexities arose analysis, skimmed overThis chapter investigate complexities","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"cowplot\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"patchwork\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"dplyr\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"Sleuth3\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(Sleuth3)"},{"path":"linear-model-complexities.html","id":"analysis-of-variance-for-balanced-designs","chapter":"14 Linear Model Complexities","heading":"14.2 Analysis of variance for balanced designs","text":"dataset used one earlier chapters balanced design:Fit linear model view anova output:model asks effect light, fertilizer, interaction effect twoEffects ANOVA tables assessed sequentially order laid modelThe model rewritten reorder treatments, balanced design equal numbers replicates among treatments values sums squares affected order terms model formulaThe balanced design ensures orthogonality, means two explanatory variables can independently assessed. words, effects depend ","code":"\nurlfile=\"https://raw.githubusercontent.com/apicellap/data/main/Data_Factorial.txt\"\nbalanced<-read.table(url(urlfile), header = TRUE)\nhead(balanced) \n#>   Fert Light   FL Biomass.m2\n#> 1   F-    L- F-L-      254.2\n#> 2   F-    L- F-L-      202.0\n#> 3   F-    L- F-L-      392.4\n#> 4   F-    L- F-L-      455.3\n#> 5   F-    L- F-L-      359.1\n#> 6   F-    L- F-L-      386.5\n names(balanced)[names(balanced) == 'Fert'] <- 'fert_'\n names(balanced)[names(balanced) == 'Light'] <- 'light_'\n names(balanced)[names(balanced) == 'FL'] <- 'fl_'\n names(balanced)[names(balanced) == 'Biomass.m2'] <- 'biomass'\n head(balanced)\n#>   fert_ light_  fl_ biomass\n#> 1    F-     L- F-L-   254.2\n#> 2    F-     L- F-L-   202.0\n#> 3    F-     L- F-L-   392.4\n#> 4    F-     L- F-L-   455.3\n#> 5    F-     L- F-L-   359.1\n#> 6    F-     L- F-L-   386.5\nanova(lm(biomass ~ light_+ fert_ + fert_:light_, data = balanced))\n#> Analysis of Variance Table\n#> \n#> Response: biomass\n#>              Df Sum Sq Mean Sq F value    Pr(>F)    \n#> light_        1  96915   96915 11.3131  0.001345 ** \n#> fert_         1 319889  319889 37.3413 8.019e-08 ***\n#> light_:fert_  1  36409   36409  4.2501  0.043587 *  \n#> Residuals    60 513998    8567                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-model-complexities.html","id":"balance-and-orthogonality","chapter":"14 Linear Model Complexities","heading":"14.2.1 Balance and orthogonality","text":"experimental design said orthogonal effects response variables uncorrelated\nmakes sums squares calculated two variables independent order included model formula\nexperimental design said orthogonal effects response variables uncorrelatedThis makes sums squares calculated two variables independent order included model formulaBalanced experimental designs equal numbers replicates treatment combination ensure orthoganility - Orthangonality lost replication numbers unequal across different treatment combinationsBalanced experimental designs equal numbers replicates treatment combination ensure orthoganility - Orthangonality lost replication numbers unequal across different treatment combinationsThe independence values sums squares (everything follows) makes things simple\nreasons, balanced designs strongly recommended\nindependence values sums squares (everything follows) makes things simpleFor reasons, balanced designs strongly recommendedIt always possible design balanced experimentsIt always possible design balanced experimentsAlso, balance can lost replicates unintentionally lostAlso, balance can lost replicates unintentionally lostThe analysis unbalanced designs complicated possibleThe analysis unbalanced designs complicated possible","code":""},{"path":"linear-model-complexities.html","id":"analysis-of-variance-with-unbalanced-designs","chapter":"14 Linear Model Complexities","heading":"14.3 Analysis of variance with unbalanced designs","text":"dataset comes paper Shaw & Mitchell-Olds (1993). dataset:\nResponse variable - final plant height\nExplanatory variable #1 - removal plant neighbors\nExplanatory variable #2 - initial size target plants\nResponse variable - final plant heightExplanatory variable #1 - removal plant neighborsExplanatory variable #2 - initial size target plantsBoth explanatory variables factors, two levels\nInitial size levels = small large\ndesign one fully factorialized four possible combinations treatments\nInitial size levels = small largeThe design one fully factorialized four possible combinations treatmentsThe design unbalanced non-orthogonal different treatment combinations different numbers biological replicatesWe create dataframe:Summarize two factor design table:model fits treatment first produces following anova table:Reverse order model formula (put size first):good news two models’ tables:\nSum Sq value size:treatment row Residuals row tables\nSum Sq value size:treatment row Residuals row tablesThe bad news:\nSum Sq values two main effects different table\nremains case test interactive effect removed model formula (terms reverse order):\n\nSum Sq values two main effects different table\nremains case test interactive effect removed model formula (terms reverse order):\nremains case test interactive effect removed model formula (terms reverse order):models can see high F value associated size term\nTherefore, means related response variable\nTherefore, means related response variableHowever, initial size explanatory variable interest experiment\nSize covariate added model control differences initial size (initial size replicates beginning experiment, wouldn’t accounted th model)\nSize covariate added model control differences initial size (initial size replicates beginning experiment, wouldn’t accounted th model)explanatory variable whether plant removal one supposed addressed experimentUnfortunately lack balance outsized effect Sum squares values treatmentt two modelslm(height ~ treatment + size, data = unbalanced)\ntreatment low F-value high P-value\ntreatment low F-value high P-value`lm(height ~ size + treatment, data = unbalanced)\ntreatment much higher F-value small p-value\ntreatment much higher F-value small p-valueNow, problem two models produce different results\nsolution just pick model two makes sense data\nauthor says model controls size assess effect treatment appropriate one case\nsays whole objective including covariate initial sisze adjust effects addressing effect treatment\nsolution just pick model two makes sense dataThe author says model controls size assess effect treatment appropriate one caseHe says whole objective including covariate initial sisze adjust effects addressing effect treatment","code":"\nheight <-  c(50, 57, 91, 94, 102, 110, 57, 71, 85, 105, 120)  \nsize <-  c(rep(\"Small\", 2), rep(\"Large\", 4), rep(\"Small\", 3),  rep(\"Large\", 2))  \ntreatment <- c(rep(\"Control\", 6), rep(\"Removal\", 5))  \nunbalanced <- data.frame(height, size, treatment)  \nunbalanced \n#>    height  size treatment\n#> 1      50 Small   Control\n#> 2      57 Small   Control\n#> 3      91 Large   Control\n#> 4      94 Large   Control\n#> 5     102 Large   Control\n#> 6     110 Large   Control\n#> 7      57 Small   Removal\n#> 8      71 Small   Removal\n#> 9      85 Small   Removal\n#> 10    105 Large   Removal\n#> 11    120 Large   Removal\ntable(size,treatment)\n#>        treatment\n#> size    Control Removal\n#>   Large       4       2\n#>   Small       2       3\nmod_TxS <- lm(height ~ treatment * size, data = unbalanced)\nanova(mod_TxS)\n#> Analysis of Variance Table\n#> \n#> Response: height\n#>                Df Sum Sq Mean Sq F value    Pr(>F)    \n#> treatment       1   35.3    35.3  0.3309 0.5831478    \n#> size            1 4846.0  4846.0 45.3658 0.0002686 ***\n#> treatment:size  1   11.4    11.4  0.1068 0.7533784    \n#> Residuals       7  747.8   106.8                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmod_SxT <- lm(height ~ size * treatment, data = unbalanced)\nanova(mod_SxT)\n#> Analysis of Variance Table\n#> \n#> Response: height\n#>                Df Sum Sq Mean Sq F value    Pr(>F)    \n#> size            1 4291.2  4291.2 40.1718 0.0003896 ***\n#> treatment       1  590.2   590.2  5.5249 0.0510495 .  \n#> size:treatment  1   11.4    11.4  0.1068 0.7533784    \n#> Residuals       7  747.7   106.8                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmod_TS <- lm(height ~ treatment + size, data = unbalanced)\nanova(mod_TS)\n#> Analysis of Variance Table\n#> \n#> Response: height\n#>           Df Sum Sq Mean Sq F value    Pr(>F)    \n#> treatment  1   35.3    35.3  0.3725    0.5586    \n#> size       1 4846.0  4846.0 51.0676 9.746e-05 ***\n#> Residuals  8  759.2    94.9                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmod_ST <- lm(height ~ size + treatment, data = unbalanced)\nanova(mod_ST)\n#> Analysis of Variance Table\n#> \n#> Response: height\n#>           Df Sum Sq Mean Sq F value    Pr(>F)    \n#> size       1 4291.2  4291.2 45.2208 0.0001489 ***\n#> treatment  1  590.2   590.2  6.2193 0.0372980 *  \n#> Residuals  8  759.2    94.9                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-model-complexities.html","id":"anova-tables-versus-coefficients-when-f-and-t-can-disagree","chapter":"14 Linear Model Complexities","heading":"14.4 ANOVA tables versus coefficients: When F and t can disagree","text":"previous section looked order dependence ANOVA tables unbalanaced, non-orthanganol designs F-tests containThis section look estimates t-tests table coefficientsHere estimates t-testts model gives priority treatment term:model gives priority intial size:contrasts sums squares ANOVA tables, estimates coefficients unaffected order explanatory variables linear model formulaThis due fact calculated sequentially\nInstead, estimate term made controlling effects variables model (including interactions)\nInstead, estimate term made controlling effects variables model (including interactions)","code":"\nsummary(mod_TS)\n#> \n#> Call:\n#> lm(formula = height ~ treatment + size, data = unbalanced)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -13.1053  -6.2105   0.8947   4.7895  14.8947 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         98.58       4.47  22.055 1.89e-08 ***\n#> treatmentRemoval    15.26       6.12   2.494   0.0373 *  \n#> sizeSmall          -43.74       6.12  -7.146 9.75e-05 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 9.741 on 8 degrees of freedom\n#> Multiple R-squared:  0.8654, Adjusted R-squared:  0.8318 \n#> F-statistic: 25.72 on 2 and 8 DF,  p-value: 0.0003281\nsummary(mod_ST)\n#> \n#> Call:\n#> lm(formula = height ~ size + treatment, data = unbalanced)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -13.1053  -6.2105   0.8947   4.7895  14.8947 \n#> \n#> Coefficients:\n#>                  Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         98.58       4.47  22.055 1.89e-08 ***\n#> sizeSmall          -43.74       6.12  -7.146 9.75e-05 ***\n#> treatmentRemoval    15.26       6.12   2.494   0.0373 *  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 9.741 on 8 degrees of freedom\n#> Multiple R-squared:  0.8654, Adjusted R-squared:  0.8318 \n#> F-statistic: 25.72 on 2 and 8 DF,  p-value: 0.0003281"},{"path":"linear-model-complexities.html","id":"marginality-of-main-effects-and-interactions","chapter":"14 Linear Model Complexities","heading":"14.5 Marginality of main effects and interactions","text":"Another glossed issue including exlcuding interaction term model isn’t significantThe convention used model simplified remove unsupported interactionsMore recently, thinking changed maintain interaction terms whether important notIn chapter 13, extended typical ANCOVA general linear model added third variable, author skipped important complexity:\nimportance main effects varying depending whether interactions present model \nimportance main effects varying depending whether interactions present model notWe revisit example now:Interestingly, complexity doesn’t necessarily arise three variables model occurs model just two variables (effects water stress pollutant (\\(SO_2\\)) stress)Begin table coefficients:Starting bottom table,\nsupport interaction two variables\nisn’t clear evidence support main effects either\nLooking table alone present idea effect either variable alone combination soya bean yield variety\nsupport interaction two variablesThere isn’t clear evidence support main effects eitherLooking table alone present idea effect either variable alone combination soya bean yield varietyHowever, ANOVA table summary analysis presents different picture data:ANOVA table output\nANOVA table also lacks support interaction effect\nHowever, F-tests main effects water \\(SO_2\\) now support effects variables soya bean yield\none-way anova Darwin’s Maize data anova table f-tests summary table t-tests produced identical p values\nHowever,summary anova tables situation different values incongruent\nANOVA table also lacks support interaction effectHowever, F-tests main effects water \\(SO_2\\) now support effects variables soya bean yieldWhen one-way anova Darwin’s Maize data anova table f-tests summary table t-tests produced identical p valuesHowever,summary anova tables situation different values incongruentThere various situations ANOVA table F-tests summary table t-tests can differ\nEquivalency require orthogonal designs, experimental design balanced can’t reason mismatched results\nAdditionally, result model affected order terms (data shown)\ncase, results incongruent summary table anova table aren’t presenting tests despite similiar row names\nEquivalency require orthogonal designs, experimental design balanced can’t reason mismatched resultsAdditionally, result model affected order terms (data shown)case, results incongruent summary table anova table aren’t presenting tests despite similiar row namesANOVA table\nFirst row - main effect \\(SO_2\\)\nSecond row - main effect water stress\nThird row - interaction\nFourth row - residual error\nFirst row - main effect \\(SO_2\\)Second row - main effect water stressThird row - interactionFourth row - residual errorSummary table coefficients:\nFirst row - intercept regression \\(SO_2\\) water stressed treatment level\nSecond row - slope regression first row\nThird row - difference slope well watered treatment\nFourth row - difference intercept well watered treatment\nFirst row - intercept regression \\(SO_2\\) water stressed treatment levelSecond row - slope regression first rowThird row - difference slope well watered treatmentFourth row - difference intercept well watered treatmentIn summary, tables row labled ‘SO2’\nANOVA table, SO2 row quantifies much variance explained sulfure dioxide gradient F-value gives ratio variance relative unexplained noise\nsummary table, SO2 row estimates slope regression yield sulfur dioxide concentration well-watered treatment adjusting effects terms model\nANOVA table, SO2 row quantifies much variance explained sulfure dioxide gradient F-value gives ratio variance relative unexplained noiseIn summary table, SO2 row estimates slope regression yield sulfur dioxide concentration well-watered treatment adjusting effects terms modelWhen analyses simple, tables produce identical resultsBut complexities non-orthagonality presence interaction (), two tables perform tests; thereby, producing differing results respective tablesUnderstanding models explaining ambiguities\nanova table, see support interaction effect\nHowever, ideally manifest F-value 1 absolutely interaction\n\ncase, F value much smaller 1\nanova table, see support interaction effect\nHowever, ideally manifest F-value 1 absolutely interaction\nHowever, ideally manifest F-value 1 absolutely interactionIn case, F value much smaller 1This example negative variance component (NVC)\ntwo interpretations:\nNVC indicate something wrong data analysis working expected\nAlso, small variance underestimate due smapling variation\n\ntwo interpretations:\nNVC indicate something wrong data analysis working expected\nAlso, small variance underestimate due smapling variation\nNVC indicate something wrong data analysis working expectedAlso, small variance underestimate due smapling variationThe author says design experiment looks appropriate analysis solid must latter reason sampling variationIf remove interaction term model formula, find anova table table coefficients come agreement:Now tables show support main effects \\(SO_2\\) stress water stress, biologically sensibleNow tables show support main effects \\(SO_2\\) stress water stress, biologically sensibleThis example shows retaining unimportant interactions model can lead missing importance main effectsThis example shows retaining unimportant interactions model can lead missing importance main effectsTo safe, different models analysed test without interaction effectsTo safe, different models analysed test without interaction effectsIt’s overall better greater sample sizes even important greater sample size provide greater power detect main effects even presence non significant interactionsIt’s overall better greater sample sizes even important greater sample size provide greater power detect main effects even presence non significant interactionsRetaining non significant interactions can also depend experiment’s aim\nfocus hypothesis testing, simplifying minimal adequate model may consistent original aims\nRetaining non significant interactions can also depend experiment’s aimWhen focus hypothesis testing, simplifying minimal adequate model may consistent original aimsIn conclusion, balanced, orthoganol designs highly recommended avoid complexities ambiguitiesIn conclusion, balanced, orthoganol designs highly recommended avoid complexities ambiguities","code":"\nxlabel <- expression(paste(\"Ozone (µL • L\" ^ \"-1\", \")\"))\nylabel <- expression(paste(\"Log Yield (kg • ha\" ^ \"-1\", \")\")) \nfig13_0 <- ggplot(case1402, aes(x= SO2, \n                                y = log(William))) + #log transform the y values \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  ggtitle(\"Yield of 'William' soya beans under diverse stress conditions\") +\n  xlab(xlabel) + ylab(ylabel) + \nscale_x_continuous(limits = c(0, 0.06), #set x axis range \n                    breaks = seq(0,0.06, by = 0.01)) +  #x axis range and increments \n  facet_wrap(vars(Stress)) + theme_bw()\nfig13_0\n#> `geom_smooth()` using formula 'y ~ x'\nsummary(lm(log(William) ~ SO2 * Stress, data = case1402))\n#> \n#> Call:\n#> lm(formula = log(William) ~ SO2 * Stress, data = case1402)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.33175 -0.17513 -0.00192  0.17254  0.39154 \n#> \n#> Coefficients:\n#>                        Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)             8.16740    0.08926  91.499   <2e-16\n#> SO2                    -3.41525    2.44456  -1.397    0.174\n#> StressWell-watered      0.19703    0.12624   1.561    0.131\n#> SO2:StressWell-watered -0.71339    3.45713  -0.206    0.838\n#>                           \n#> (Intercept)            ***\n#> SO2                       \n#> StressWell-watered        \n#> SO2:StressWell-watered    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2362 on 26 degrees of freedom\n#> Multiple R-squared:  0.2585, Adjusted R-squared:  0.173 \n#> F-statistic: 3.022 on 3 and 26 DF,  p-value: 0.04769\nanova(lm(log(William) ~ SO2 * Stress, data = case1402))\n#> Analysis of Variance Table\n#> \n#> Response: log(William)\n#>            Df  Sum Sq  Mean Sq F value  Pr(>F)  \n#> SO2         1 0.26558 0.265581  4.7617 0.03833 *\n#> Stress      1 0.23764 0.237642  4.2608 0.04911 *\n#> SO2:Stress  1 0.00238 0.002375  0.0426 0.83812  \n#> Residuals  26 1.45014 0.055775                  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(lm(log(William) ~ SO2 + Stress, data = case1402))\n#> \n#> Call:\n#> lm(formula = log(William) ~ SO2 + Stress, data = case1402)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.3222 -0.1711 -0.0051  0.1737  0.3892 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)         8.17692    0.07507 108.920   <2e-16 ***\n#> SO2                -3.77195    1.69764  -2.222   0.0349 *  \n#> StressWell-watered  0.17800    0.08469   2.102   0.0450 *  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.2319 on 27 degrees of freedom\n#> Multiple R-squared:  0.2573, Adjusted R-squared:  0.2023 \n#> F-statistic: 4.677 on 2 and 27 DF,  p-value: 0.01803\nanova(lm(log(William) ~ SO2 + Stress, data = case1402))\n#> Analysis of Variance Table\n#> \n#> Response: log(William)\n#>           Df  Sum Sq  Mean Sq F value  Pr(>F)  \n#> SO2        1 0.26558 0.265581  4.9367 0.03486 *\n#> Stress     1 0.23764 0.237642  4.4174 0.04504 *\n#> Residuals 27 1.45252 0.053797                  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"generalized-linear-models-glms.html","id":"generalized-linear-models-glms","chapter":"15 Generalized Linear Models (GLMs)","heading":"15 Generalized Linear Models (GLMs)","text":"","code":""},{"path":"generalized-linear-models-glms.html","id":"glms","chapter":"15 Generalized Linear Models (GLMs)","heading":"15.1 GLMs","text":"Chapter 7 used normal least squares perform linear regression analysis using lm() functionThe simple linear regression adequately accomodate curvature relationship wood density hardness\nresult, normal least squares assumption constant variance infringed\nresult, normal least squares assumption constant variance infringedA model greater degree flexibility can model mean variance separately - GLMs doThis chapter uses GLMs maximum likelihood methods GLMs based extend linear regression example new ways","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"cowplot\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"patchwork\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"dplyr\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"Sleuth3\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"SemiPar\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggfortify\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"MASS\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(Sleuth3)\nlibrary(SemiPar)\nlibrary(ggfortify)\nlibrary(MASS)"},{"path":"generalized-linear-models-glms.html","id":"the-trouble-with-transformations","chapter":"15 Generalized Linear Models (GLMs)","heading":"15.2 The trouble with transformations","text":"chapter 7, created following model:looked diagnostic plots, found model wasn’t completely appropriate dataTwo key issues model :\ncurvature relationship captured linear regression\nVariance constant - increases mean\ncurvature relationship captured linear regressionVariance constant - increases meanOne way deal problems transform response variable data\ndeal upward bend relationship, square root transformation can applied\nreduce higher values closer smaller ones, may get data fall closer straight line relationship density\ndeal upward bend relationship, square root transformation can appliedThis reduce higher values closer smaller ones, may get data fall closer straight line relationship densityDirectly transform data model:Compare regression lines graphs:transformation improves linearity (slightly, correlation (R) value actually improved 1.1%), residuals still increase mean:Another common transformation assessed log (case natural log) transformation:natural log transformation corrects issue causes curvature opposite directionThough log transformation correct variance constant:basically log transformation solves variance issue, creates another issue relationshipThe log transformation can taken step using polynomial regression trades linear term quadratic one (\\(density^2\\))\nNote must done using () function:\nNote must done using () function:quadratic polynomial regression converts straight line relationship one simple curve:log transformed data combined polynomial regression address curved relataionship wood density hardness rectifies variance issues, model much complex simple linear regressionWhile log transformed data combined polynomial regression address curved relataionship wood density hardness rectifies variance issues, model much complex simple linear regressionPolynomial regressions often extrapolate poorly beyond range dataPolynomial regressions often extrapolate poorly beyond range dataTransformations summed :\nsquare root transformation provided linear relationship variance constant (increased mean)\nlog transformation produced constant variance overcorrected introduced curvature relationship opposite direction\nTransformations summed :square root transformation provided linear relationship variance constant (increased mean)log transformation produced constant variance overcorrected introduced curvature relationship opposite direction","code":"\ndata(janka)\njanka.ls1 <- lm(hardness ~ dens, data = janka)\njanka.sqrt <- lm(sqrt(hardness) ~ dens, data = janka)\nfig15_1a <- ggplot(janka, aes(x = dens, y = hardness)) + geom_point() + \n  geom_smooth(method = \"lm\") \nfig15_1a\n#> `geom_smooth()` using formula 'y ~ x'\n\nfig15_1b <- ggplot(janka, aes(x = dens, y = sqrt(hardness))) + geom_point() + \n  geom_smooth(method = \"lm\") \nfig15_1b\n#> `geom_smooth()` using formula 'y ~ x'\nfig7_5 <- autoplot(janka.ls1, which = c(1,3), ncol =2) + labs(caption = \"Original data\")\nfig7_5 #this is for the diagnostics on the original model \n\nfig15_2 <- autoplot(janka.sqrt, which = c(1,3), ncol =2) + labs(caption = \"Transformed data\")\nfig15_2\njanka.log <- lm(log(hardness) ~ dens, data = janka)\nfig15_3a <- ggplot(janka, aes(x = dens, y = hardness)) + geom_point() + \n  geom_smooth(method = \"lm\") \nfig15_3a\n#> `geom_smooth()` using formula 'y ~ x'\n\nfig15_3b <- ggplot(janka, aes(x = dens, y = log(hardness))) + geom_point() + \n  geom_smooth(method = \"lm\") + \n  geom_smooth(se = FALSE, colour = \"red\", linetype = \"dashed\")\nfig15_3b\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nfig7_5 <- autoplot(janka.ls1, which = c(1,3), ncol =2) + labs(caption = \"Original data\")\nfig7_5 #this is for the diagnostics on the original model \n\nfig15_4 <- autoplot(janka.log, which = c(1,3), ncol =2) + labs(caption = \"Log transformed data\")\nfig15_4\njanka.quad <- lm(log(hardness) ~ dens + I(dens^2), data = janka)\nfig15_5a <- ggplot(janka, aes(x = dens, y = hardness)) + geom_point() + \n  geom_smooth(method = \"lm\") \nfig15_5a\n#> `geom_smooth()` using formula 'y ~ x'\n\nfig15_5b <- ggplot(janka, aes(x = dens, y = log(hardness))) + geom_point() + \n  geom_smooth(method = \"lm\", formula = y ~ x + I(x^2)) \nfig15_5b\nfig7_5 <- autoplot(janka.ls1, which = c(1,3), ncol =2) + labs(caption = \"Original data\")\nfig7_5 #this is for the diagnostics on the original model \n\nfig15_6 <- autoplot(janka.quad, which = c(1,3), ncol =2) + labs(caption = \"Log transformed polynomial data\")\nfig15_6"},{"path":"generalized-linear-models-glms.html","id":"the-box-cox-power-transform","chapter":"15 Generalized Linear Models (GLMs)","heading":"15.3 The Box-Cox power transform","text":"alternative transformation called Box-Cox family transformations","code":""},{"path":"generalized-linear-models-glms.html","id":"the-box-cox-family-of-transformations","chapter":"15 Generalized Linear Models (GLMs)","heading":"15.3.1 The Box-Cox family of transformations","text":"Implemented boxcox() function MASS packageThe transformation performed raising data power (lambda) varied different levels\nlambda = 2, data squared\nlambda = 0.5, data square rooted\nlambda = 1, data transformed\nlambda = 0, natural log used\nlambda = 2, data squaredWhen lambda = 0.5, data square rootedWhen lambda = 1, data transformedWhen lambda = 0, natural log usedThe R output boxcox() plots maximum likelihood maximum likelihood based 95% confidence interval","code":""},{"path":"generalized-linear-models-glms.html","id":"maximum-likelihood","chapter":"15 Generalized Linear Models (GLMs)","heading":"15.3.2 Maximum likelihood","text":"Maximum likelihood finds values parameters statistical model likely reproduce observed data (finds best fit model data)\nsimiliar least squares finds line best fit minimizing sum squared differences\nsimiliar least squares finds line best fit minimizing sum squared differencesAnd far data normally distributed errors, corresponding form maximum likelihood analysis normal least squares already knownHowever, normal distribution good model certain forms data (.e. binomial data)generalized linear models come can make use flavors maximum likelihood (ML) can applied broader range dataThese flavors ML can make use general approach calculation underpins GLMs known iterative weighted least squares (IWLS)IWLS can thought analogous normal least squares approach already familiar withIn contrast calculations involved normal least squares generate exact solutions, ML likelihood methods iterative approximate\nwords, means ML methods generate many different potential lines best fit using algorithms graduately hone actual best solution\nwords, means ML methods generate many different potential lines best fit using algorithms graduately hone actual best solutionDetermine transformation use:figure, box-cox method applied model determine type transformation suits data appropriately\ncomparing lambda values -log-likelihood scale\nGiven corresponding -log-likelihood value 95% confidence interval, respective x (lambda) value transformation use\nSince janka.ls1 model, lambda = 0.5, response variable data must square rooted\nInstead outright square root transformation, GLM equivalent\n\ncomparing lambda values -log-likelihood scaleGiven corresponding -log-likelihood value 95% confidence interval, respective x (lambda) value transformation useSince janka.ls1 model, lambda = 0.5, response variable data must square rooted\nInstead outright square root transformation, GLM equivalent\nInstead outright square root transformation, GLM equivalent","code":"\nfig15_6 <- boxcox(janka.ls1)"},{"path":"generalized-linear-models-glms.html","id":"generalized-linear-models-in-r","chapter":"15 Generalized Linear Models (GLMs)","heading":"15.4 Generalized linear models in R","text":"GLMs three components\nlinear predictor\nlinear predictor follows tilde (~) lm() function formula\njanka data, linear predictor wood density (dens)\nvariance function\naspect models variation data\nsimilar normal least squares uses normal distribution model residual variation\ndifference case GLMs limited normal distribution\nGLMs can use several types distributions including Poisson, binomial, gamma distributions\n\n\nlink function\naspect plays role equivalent transformation data normal least squares models\ninstead transforming data, predictions mad linear predictor transformed\nCommon link functions include log, square root, logistic functions\nlinear predictorThe linear predictor follows tilde (~) lm() function formulaIn janka data, linear predictor wood density (dens)variance functionThis aspect models variation data\nsimilar normal least squares uses normal distribution model residual variation\ndifference case GLMs limited normal distribution\nGLMs can use several types distributions including Poisson, binomial, gamma distributions\n\nsimilar normal least squares uses normal distribution model residual variationThe difference case GLMs limited normal distribution\nGLMs can use several types distributions including Poisson, binomial, gamma distributions\nGLMs can use several types distributions including Poisson, binomial, gamma distributionsA link functionThis aspect plays role equivalent transformation data normal least squares modelsBut instead transforming data, predictions mad linear predictor transformedCommon link functions include log, square root, logistic functionsCreate GLM janka data:Take look output original model:Take look output new GLM model:GLM analysis deviance (ANODEV) table\nanova(janka.ml1) produces type table ANOVA table\nDeviance related maximum likelihood\ngeneralized version sum squares\ndefined terms comparing pairs models one complex simpler version\nDeviance 2x difference log-likelihoods complex model, follows chi-squared distribution\n\ngeneralized equivalent sum squares density deviance residual deviance bottom row\ntop row ANODEV contains total degrees freedom total deviance\nANODEV contain mean square values, statistics, p-values\nanova(janka.ml1) produces type table ANOVA tableDeviance related maximum likelihood\ngeneralized version sum squares\ndefined terms comparing pairs models one complex simpler version\nDeviance 2x difference log-likelihoods complex model, follows chi-squared distribution\ngeneralized version sum squaresIt defined terms comparing pairs models one complex simpler versionDeviance 2x difference log-likelihoods complex model, follows chi-squared distributionThe generalized equivalent sum squares density deviance residual deviance bottom rowThe top row ANODEV contains total degrees freedom total devianceThe ANODEV contain mean square values, statistics, p-valuesThe box-cox (normal least squares) analysis showed square root transformation produce best fitting modelWe can use information tweak GLMOne snag already know variance increased mean increasedTo circumvent issue, can square root transformation (’ll use square root link function) combination distribution variance increases mean\ndistribution meets need gamma distribution\ndistribution meets need gamma distributionFit GLM square root link function model data gamma distribution:Draw graph resulting model:Derive regression point estimates intercept slope values:Get confidence intervals:CIs GLMs calculated using maximum likelihood methods called likelihood profile intervals appropriate GLMs\nSince intervals based normal distribution, need symmetric\nSince intervals based normal distribution, need symmetric","code":"\njanka.ml1 <- glm(hardness ~ dens, data = janka, \n                 family = gaussian(   #using the Gaussian/normal distribution as a model for the residual variability (this is the default)\n                   link = \"identity\")) #used to model the mean. No transformation is being performed on the data (this is the default)\nanova(janka.ls1)\n#> Analysis of Variance Table\n#> \n#> Response: hardness\n#>           Df   Sum Sq  Mean Sq F value    Pr(>F)    \n#> dens       1 21345674 21345674  636.98 < 2.2e-16 ***\n#> Residuals 34  1139366    33511                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(janka.ml1)\n#> Analysis of Deviance Table\n#> \n#> Model: gaussian, link: identity\n#> \n#> Response: hardness\n#> \n#> Terms added sequentially (first to last)\n#> \n#> \n#>      Df Deviance Resid. Df Resid. Dev\n#> NULL                    35   22485041\n#> dens  1 21345674        34    1139366\njanka.gamma <- glm(hardness ~ dens, data = janka,\n                   family = Gamma(link = \"sqrt\"))\nfig15_7 <- ggplot(janka, aes(x = dens, y = hardness)) + \n  labs(x= \"Density\", y = \"Hardness\") + \n  ggtitle(\"GLM, square root link func, Gamma variance\") + \n  geom_point() +\n  geom_smooth(method = \"glm\", method.args = list(Gamma(link= \"sqrt\")))\nfig15_7\n#> `geom_smooth()` using formula 'y ~ x'\ncoef(janka.gamma)\n#> (Intercept)        dens \n#>   1.8672884   0.7677963\nconfint(janka.gamma)\n#> Waiting for profiling to be done...\n#>                  2.5 %    97.5 %\n#> (Intercept) 0.09706551 3.6699483\n#> dens        0.72361627 0.8122638\ncoefplot(janka.gamma)"},{"path":"glms-for-count-data.html","id":"glms-for-count-data","chapter":"16 GLMs for Count Data","heading":"16 GLMs for Count Data","text":"","code":""},{"path":"glms-for-count-data.html","id":"introduction-9","chapter":"16 GLMs for Count Data","heading":"16.1 Introduction","text":"Count data integers (.e. whole numbers) numbers individuals, species, times event occured etc.starting point count data GLM use Poisson distribution log link function\nlog link function makes sure predicted counts positive taking exponential values generated linear predictor\nlog link function makes sure predicted counts positive taking exponential values generated linear predictorIn Poisson distribution, variance equal mean, property must examined implementing modelCount data good fits Poisson distribution usually many 0s small valuesPoisson isn’t necessarily good fit every type count dataAs mean Poisson distribution increases, distribution converges towards normal distribution","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"dplyr\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggfortify\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"readr\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggfortify)\nlibrary(readr)"},{"path":"glms-for-count-data.html","id":"glms-for-count-data-1","chapter":"16 GLMs for Count Data","heading":"16.2 GLMs for count data","text":"example dataset encompasses counts grassland plant species relation nitrogen depositionIt factorial designed experiment examines plant diversity changes following nitrogen pollution\nnitrogen variable continuous\nspecies count data integers\nnitrogen variable continuousThe species count data integersLoad create dataframe:Visualize data:Data seem support idea negative relationship input nitrogen grassland environment plant diversityHowever, linear regression higher nitrogen levels predict negative species counts, wouldn’t make senseAdditionally, variability data higher species counts higher, linear regression great job modeling thisWe can try use Poisson GLM fit data:Check Poisson GLM appropriate data:\npoisson glm defined variance equivalent mean\nassumption met, ratio residual deviance degrees freedom (dispersion) approximately 1:1\npoisson glm defined variance equivalent meanIf assumption met, ratio residual deviance degrees freedom (dispersion) approximately 1:1According summary() output, ratio 64.707:72 (0.90)\nlevel variation expected data Poisson distributed\nratio residual variance DF 1.2, continued use Poisson distribution model data, underestimating true level variation data\nOne approach deploy data overdispersed quasi-maximum likelihood, discussed next section\n\ncalculated ratio approximately 1 (’s 0.9), suggests model appropriate dataset\nlevel variation expected data Poisson distributed\nratio residual variance DF 1.2, continued use Poisson distribution model data, underestimating true level variation data\nOne approach deploy data overdispersed quasi-maximum likelihood, discussed next section\nratio residual variance DF 1.2, continued use Poisson distribution model data, underestimating true level variation dataOne approach deploy data overdispersed quasi-maximum likelihood, discussed next sectionThe calculated ratio approximately 1 (’s 0.9), suggests model appropriate datasetThe Poisson GLM like fitting straight line log-transformed counts\nslope intercept representation table coefficients can visualized following plot:\nslope intercept representation table coefficients can visualized following plot:Produce new visualization GLM model superimposed:curvilinear relationship (linear log-y scale) predict negative species counts better job modelling higher variability higher species counts","code":"\nurlfile=\"https://raw.githubusercontent.com/apicellap/data/main/Data_species_counts.txt\"\nspecies<-read.table(url(urlfile))\nstr(species) \n#> 'data.frame':    74 obs. of  2 variables:\n#>  $ N_deposition  : num  8.56 7.7 8.28 8.14 10.99 ...\n#>  $ Species_counts: int  20 17 25 18 20 10 13 14 15 15 ...\nsummary(species)\n#>   N_deposition   Species_counts \n#>  Min.   : 7.70   Min.   : 6.00  \n#>  1st Qu.:14.26   1st Qu.:10.00  \n#>  Median :20.25   Median :13.00  \n#>  Mean   :20.58   Mean   :13.91  \n#>  3rd Qu.:27.11   3rd Qu.:15.00  \n#>  Max.   :40.86   Max.   :27.00\nfig16_1 <- ggplot(species, aes(x =N_deposition, y = Species_counts )) + geom_point() + \n  geom_smooth(method = \"lm\")\nfig16_1\n#> `geom_smooth()` using formula 'y ~ x'\nglm1 <- glm(Species_counts ~ N_deposition, \n            family = poisson(link = \"log\"), data = species)\nsummary(glm1)\n#> \n#> Call:\n#> glm(formula = Species_counts ~ N_deposition, family = poisson(link = \"log\"), \n#>     data = species)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.1788  -0.6813  -0.1904   0.5825   3.1749  \n#> \n#> Coefficients:\n#>               Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   3.210340   0.080655  39.803  < 2e-16 ***\n#> N_deposition -0.029436   0.003975  -7.405 1.31e-13 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 120.778  on 73  degrees of freedom\n#> Residual deviance:  64.707  on 72  degrees of freedom\n#> AIC: 396.2\n#> \n#> Number of Fisher Scoring iterations: 4\nfig16_1b <- ggplot(species, aes(x =N_deposition, y = log(Species_counts) )) + geom_point() + \n  geom_smooth(method = \"lm\")\nfig16_1b\n#> `geom_smooth()` using formula 'y ~ x'\nfig16_2 <- ggplot(species, aes(x =N_deposition, y = Species_counts )) + geom_point() + \n  stat_smooth(method = \"glm\", method.args = list(family = \"poisson\"))\nfig16_2\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"glms-for-count-data.html","id":"quasi-maximum-likelihood","chapter":"16 GLMs for Count Data","heading":"16.3 Quasi-maximum likelihood","text":"Classical maximum likelihood assumes level variability approximately predicted distribution appliedOn hand, quasi-maximum likelihood estimates observed level variation data adjusts standard errors accordinglyComparing summary(glm1) summary(qlm1) find estimates slope intercept , standard errors slightly different\nSince level variation approximately expect Poisson distribution, changes SEs small\nHowever, dispersion index value greater 1.2, modification model using quasi maximum likelihood bring larger changes standard errors intercept slope values\nSince level variation approximately expect Poisson distribution, changes SEs smallHowever, dispersion index value greater 1.2, modification model using quasi maximum likelihood bring larger changes standard errors intercept slope valuesWe can look diagnostic plots linear regression model take closer look residual variation:left right fan shapes residuals indicate increase variance mean species count increases instead constant level variation, assumed linear regressionLog transformation response variable rectify variance:type log transformation applied Poisson GLM log link function\nusing poisson distribution, expect variation increase mean variation supposed equal mean\nuse function GLM means model can produce negative values species counts high nitrogen deposition levels\nusing poisson distribution, expect variation increase mean variation supposed equal meanThe use function GLM means model can produce negative values species counts high nitrogen deposition levelsThe GLM better job modelling patterns mean variance separately using one sized fits approach transforming data","code":"\nqlm1 <- glm(Species_counts ~ N_deposition, \n            family = quasipoisson, data = species)\nsummary(qlm1)\n#> \n#> Call:\n#> glm(formula = Species_counts ~ N_deposition, family = quasipoisson, \n#>     data = species)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.1788  -0.6813  -0.1904   0.5825   3.1749  \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   3.210340   0.078168   41.07  < 2e-16 ***\n#> N_deposition -0.029436   0.003853   -7.64 7.18e-11 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for quasipoisson family taken to be 0.9392865)\n#> \n#>     Null deviance: 120.778  on 73  degrees of freedom\n#> Residual deviance:  64.707  on 72  degrees of freedom\n#> AIC: NA\n#> \n#> Number of Fisher Scoring iterations: 4\nfig16_3 <- autoplot(lm(Species_counts ~ N_deposition, data = species), \n                    which = c(1,3))\nfig16_3\nautoplot(lm(log(Species_counts) ~ N_deposition, data = species), \n                    which = c(1,3))"},{"path":"binomial-glms.html","id":"binomial-glms","chapter":"17 Binomial GLMs","heading":"17 Binomial GLMs","text":"","code":""},{"path":"binomial-glms.html","id":"binomial-counts-and-proportion-data","chapter":"17 Binomial GLMs","heading":"17.1 Binomial counts and proportion data","text":"Binomial counts arise data comprised variables known number occasions (binomial trials) something interest occur\nknow many times something happened (successes) many times (failures)\nsucesses + failures sum give binomial denominator\n\nknow many times something happened (successes) many times (failures)\nsucesses + failures sum give binomial denominator\nsucesses + failures sum give binomial denominatorBinomial count data can expressed proportions (, proportion successes)binomial denominator (number trials) varies better use binomial count data trials weighted equallyWhen size trials recorded, proportion version data must used","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"dplyr\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"AICcmodavg\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(AICcmodavg)"},{"path":"binomial-glms.html","id":"the-beetle-data","chapter":"17 Binomial GLMs","heading":"17.2 The beetle data","text":"chapter, focus data experiment examined mortality rates eight batches flour beetle Tribolium confusa groups exposed different doses pesticide (carbon disulfide) 1935Insecticide treatment lasted 5 hoursThe unit Dose \\(mg \\cdot L^{-1}\\)Change column names:Add column number individuals survived pesticide treatment:binomial GLM investigate number beetles killed (successes) number tested (binomial denominator) function dose insecticide (concentration \\(mg \\cdot L^{-1}\\))binomial count data expressed proportions, bounded (0,1)\nfloor ceiling values constrain variance\nfloor ceiling values constrain varianceThe expected means require form S-shaped relationship model mean\nvariance decreases towards extremes (0 1) greatest \nvariance decreases towards extremes (0 1) greatest betweenA GLM using binomial distribution logistic curve (default link function) models mean symmetric S-shape","code":"\ndata(\"beetle\")\nbeetle\n#>    Dose Number_tested Number_killed Mortality_rate\n#> 1 49.06            49             6      0.1224490\n#> 2 52.99            60            13      0.2166667\n#> 3 56.91            62            18      0.2903226\n#> 4 60.84            56            28      0.5000000\n#> 5 64.76            63            52      0.8253968\n#> 6 68.69            59            53      0.8983051\n#> 7 72.61            62            61      0.9838710\n#> 8 76.54            60            60      1.0000000\nnames(beetle)[2] <- \"tested\"\nnames(beetle)[3] <- \"killed\"\nnames(beetle)\n#> [1] \"Dose\"           \"tested\"         \"killed\"        \n#> [4] \"Mortality_rate\"\nbeetle$alive <- beetle$tested - beetle$killed\nhead(beetle)\n#>    Dose tested killed Mortality_rate alive\n#> 1 49.06     49      6      0.1224490    43\n#> 2 52.99     60     13      0.2166667    47\n#> 3 56.91     62     18      0.2903226    44\n#> 4 60.84     56     28      0.5000000    28\n#> 5 64.76     63     52      0.8253968    11\n#> 6 68.69     59     53      0.8983051     6"},{"path":"binomial-glms.html","id":"logits-and-the-logistic-curve","chapter":"17 Binomial GLMs","heading":"17.2.1 Logits and the logistic curve","text":"logistic transformation converts proportions logits\nLogits natural logs odds, ratio successes failures\nbinomial denominator 10 five successes five failures, logit \\(log(5/5) = 0\\)\nLogits natural logs odds, ratio successes failuresFor binomial denominator 10 five successes five failures, logit \\(log(5/5) = 0\\)One logit worth remembering proportion 0.5 logit zero\nNegative logits correspond proportions less 0.5\nPositve logits correspond proportions greater 0.5\nNegative logits correspond proportions less 0.5Positve logits correspond proportions greater 0.5Logistic transformations map proportions zero one onto symmetric S-shaped curve asymptotes towards \\(\\pm\\infty\\)","code":""},{"path":"binomial-glms.html","id":"glm-for-binomial-counts","chapter":"17 Binomial GLMs","heading":"17.3 GLM for binomial counts","text":"GLM binomial counts analyses number beetles killed batch taking account size groupThe numbers successes failures must bound together can jointly supplied response variable binomial GLM model formulaCreate model binomial count data:Extract regression intercept slope line (linear logit scale):Zero falls way outside 95% CI, can probably reject null hypothesis dose dependent relationship beetle mortality rate:display() function provides result different form:Model formula equivalent analysis - weighted GLM proportion data:coefficents models m1 m2 sameWe can use weighted GLM help plot data:clear plot mortality rate proportion:Referring back summary() output table, see ratio residual deviance degrees freedom 8.4379:6 giving us dispersion parameter approximately 1.4\nauthor says data overdispersed, since index value rule thumb value 1.2 overdispersed data, can account using quasi-maximum likelihood approach\nauthor says data overdispersed, since index value rule thumb value 1.2 overdispersed data, can account using quasi-maximum likelihood approachQuasi-binomial approach baked model:summary output table gives specific like dispersion parameter lower previouslyThe standard errors little bit largerNo AIC value provided","code":"\nm1_logit <- glm(cbind(killed, alive) #written as (successes, failures) as we are interested in mortality rate\n                ~ Dose, data = beetle,\n                family = binomial(link = \"logit\"))\ncoef(m1_logit)\n#> (Intercept)        Dose \n#> -14.5780604   0.2455399\nconfint(m1_logit)\n#> Waiting for profiling to be done...\n#>                   2.5 %      97.5 %\n#> (Intercept) -17.2645230 -12.1608424\n#> Dose          0.2056099   0.2900912\ndisplay(m1_logit)\n#> glm(formula = cbind(killed, alive) ~ Dose, family = binomial(link = \"logit\"), \n#>     data = beetle)\n#>             coef.est coef.se\n#> (Intercept) -14.58     1.30 \n#> Dose          0.25     0.02 \n#> ---\n#>   n = 8, k = 2\n#>   residual deviance = 8.4, null deviance = 267.7 (difference = 259.2)\nm2 <- glm(Mortality_rate ~ Dose, data = beetle,family = binomial, #mortality rate as a function of dosage\n          weight = tested) #ensures that group size is taken into account \nsummary(m2)\n#> \n#> Call:\n#> glm(formula = Mortality_rate ~ Dose, family = binomial, data = beetle, \n#>     weights = tested)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -1.3456  -0.4515   0.7929   1.0422   1.3262  \n#> \n#> Coefficients:\n#>              Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept) -14.57806    1.29846  -11.23   <2e-16 ***\n#> Dose          0.24554    0.02149   11.42   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 267.6624  on 7  degrees of freedom\n#> Residual deviance:   8.4379  on 6  degrees of freedom\n#> AIC: 38.613\n#> \n#> Number of Fisher Scoring iterations: 4\nfig17_2 <- ggplot(beetle, aes(x = Dose, y = Mortality_rate)) + \n  xlab(\"Dose (mg/L)\") + ylab(\"Mortality rate\") + \n  geom_point() + \n   geom_smooth(method = \"glm\", method.args = list(binomial), aes(weight = tested)) + \n  scale_x_continuous(limits = c(45, 80),\n                     breaks = seq(45, 80, by = 5))\nfig17_2\n#> `geom_smooth()` using formula 'y ~ x'\nm1_quasi <- glm(cbind(killed, alive) \n                ~ Dose, data = beetle,\n                family = quasibinomial)\nsummary(m1_quasi)\n#> \n#> Call:\n#> glm(formula = cbind(killed, alive) ~ Dose, family = quasibinomial, \n#>     data = beetle)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -1.3456  -0.4515   0.7929   1.0422   1.3262  \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -14.57806    1.46611  -9.943 5.98e-05 ***\n#> Dose          0.24554    0.02427  10.118 5.42e-05 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for quasibinomial family taken to be 1.274895)\n#> \n#>     Null deviance: 267.6624  on 7  degrees of freedom\n#> Residual deviance:   8.4379  on 6  degrees of freedom\n#> AIC: NA\n#> \n#> Number of Fisher Scoring iterations: 4"},{"path":"binomial-glms.html","id":"alternative-link-functions","chapter":"17 Binomial GLMs","heading":"17.4 Alternative link functions","text":"cases, including beetle dataset, one link function can used within given distribution GLMFor binomial GLM, two common alternatives logistic function, default, include:\ncomplimentary log-log (cloglog)\nprobit\ncomplimentary log-log (cloglog)probitComplimentary log-log version m1:Probit version m1:One way approach comparing different models use information criteria (AIC)\ntake size 471, total number beetles tested\ntake size 471, total number beetles tested","code":"\nm1_cloglog <- glm(cbind(killed, alive) #written as (successes, failures) as we are interested in mortality rate\n                ~ Dose, data = beetle,\n                family = binomial(link = \"cloglog\"))\nm1_probit <- glm(cbind(killed, alive) #written as (successes, failures) as we are interested in mortality rate\n                ~ Dose, data = beetle,\n                family = binomial(link = \"probit\"))"},{"path":"binomial-glms.html","id":"model-selection-using-information-criteria","chapter":"17 Binomial GLMs","heading":"17.4.1 Model selection using information criteria","text":"AIC values one many tools help choose modelLowered AIC values preferred higher onesBecause AIC values one many tools, models shouldn’t chosen solely based upon lowest AIC valueThere usually ‘right’ model\nPresenting model way shroud complex model generating process\nbetter present 2-3 models single best one\nPresenting model way shroud complex model generating processIt better present 2-3 models single best oneInformation criteria fuzzy lines hard cuts associated levels statistical significance, makes trickier explainTwo models AIC values within ~2 units considered indistinguishableUsually models 10-20 AIC units higher (, worse) left final short list candidatesWhen choosing models similiar AIC values, ’s best narrow models favor simpler modelsCreate dataframe AIC values:probit model gives small improvement original model, considered virtually based measurementOn hand, cloglog model provides 4-5 unit improvement first model\nhovers around boundary one might prefer model others\nhovers around boundary one might prefer model othersStill models within 5 AIC units can considered equal viable alternativesBurnham & Anderson (2002) say test called multimodel inference make predictions based models\nPicking ‘best’ model reflect model selection uncertainty involved arriving model\n‘journey ’garden forking paths’\n\nPicking ‘best’ model reflect model selection uncertainty involved arriving model\n‘journey ’garden forking paths’\n‘journey ’garden forking paths’example, use AIC overkill number parameters case\nAIC variable result changes likelihood, measure goodness fit\nAIC variable result changes likelihood, measure goodness fit","code":"\ncand.models <- AIC(m1_logit, m1_probit, m1_cloglog)\ncand.models$delta_AIC <- cand.models$AIC - max(cand.models$AIC)\ncand.models\n#>            df      AIC delta_AIC\n#> m1_logit    2 38.61272  0.000000\n#> m1_probit   2 37.54547 -1.067244\n#> m1_cloglog  2 33.83604 -4.776678"},{"path":"glms-for-binary-data.html","id":"glms-for-binary-data","chapter":"18 GLMs for binary data","heading":"18 GLMs for binary data","text":"","code":""},{"path":"glms-for-binary-data.html","id":"binary-data","chapter":"18 GLMs for binary data","heading":"18.1 Binary data","text":"Binary data type binomial count data binomial denominator one\nTherefore, every trial value either one zero\nTherefore, every trial value either one zeroThis type data can analyzed using GLM binomial distribution set link functions prevent predictions zero oneHowever, since type data constrained, differences:\nratio residual deviance: DF diagnose /overdispersion apply\ncan use binnedplot() function arm package diagnose dispersion\nratio residual deviance: DF diagnose /overdispersion applyBut can use binnedplot() function arm package diagnose dispersion","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggfortify\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"patchwork\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"dplyr\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"Sleuth3\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"readr\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(ggfortify)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(Sleuth3)\nlibrary(readr)"},{"path":"glms-for-binary-data.html","id":"the-wells-data-set-for-the-binary-glm-example","chapter":"18 GLMs for binary data","heading":"18.2 The wells data set for the binary GLM example","text":"example dataset chapter come Gelman Hill (2006):data concern Bangladeshi region whcih many drinking water wells contaminated naturally occuring arsenic:dataframe consists three important variables (two ignore):\nswitch - binary response whether people switch well drinking\narsenic - concentration arsenic\ndist - distance nearest safe well\nswitch - binary response whether people switch well drinkingarsenic - concentration arsenicdist - distance nearest safe wellDefine labels:Visualize data distance safe well may predict switching probability:figure seems like distance well may influence whether someone switchesRescale x axis values:Fit GLM switching vs distance:Check model’s diagnostics:Due constrained nature binary data, diagnostic plots little valueDiagnose dispersion:gray lines plot indicate \\(\\pm2\\) standard errors within approximately 95% binned residuals lieIdeally type graph, ’d want plenty bins plenty datapoints per bin, approach works well smaller datasetsThe expectation around 95% residuals falls within bounds seems met, can proceed:Examine coefficents:Examine 95% confidence interval:CI supports hypothesis away well , less likely people switch \nGelman & Hill (2006) suggest rule thumb interpretting slope logistic regression:\n‘divide four rule’ divide coefficient logistic regression slope 4 get approximate estimate maximum predicted effect unit change predictor response\n\nGelman & Hill (2006) suggest rule thumb interpretting slope logistic regression:\n‘divide four rule’ divide coefficient logistic regression slope 4 get approximate estimate maximum predicted effect unit change predictor response\n‘divide four rule’ divide coefficient logistic regression slope 4 get approximate estimate maximum predicted effect unit change predictor responsePlot data superimposed binomial GLM:Evaluate effect arsenic concentration:Given model following figure, also seems clear positive effect higher arsenic concentrations leading greater probabilities switching wells:","code":"\nurlfile=\"https://raw.githubusercontent.com/apicellap/data/main/Data_Binary_Wells.csv\"\nwells<-read_csv(url(urlfile))\n#> Rows: 3020 Columns: 5\n#> ── Column specification ────────────────────────────────────\n#> Delimiter: \",\"\n#> dbl (5): switch, arsenic, dist, assoc, educ\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nstr(wells)\n#> spec_tbl_df [3,020 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#>  $ switch : num [1:3020] 1 1 0 1 1 1 1 1 1 1 ...\n#>  $ arsenic: num [1:3020] 2.36 0.71 2.07 1.15 1.1 3.9 2.97 3.24 3.28 2.52 ...\n#>  $ dist   : num [1:3020] 16.8 47.3 21 21.5 40.9 ...\n#>  $ assoc  : num [1:3020] 0 0 0 0 1 1 1 0 1 1 ...\n#>  $ educ   : num [1:3020] 0 0 10 12 14 9 4 10 0 0 ...\n#>  - attr(*, \"spec\")=\n#>   .. cols(\n#>   ..   switch = col_double(),\n#>   ..   arsenic = col_double(),\n#>   ..   dist = col_double(),\n#>   ..   assoc = col_double(),\n#>   ..   educ = col_double()\n#>   .. )\n#>  - attr(*, \"problems\")=<externalptr>\nxlabel <- \"Distance to nearest well\"\nylabel <- \"Probability of switching\"\nfig18_1 <- ggplot(wells, aes(x = dist, y = switch)) + \n  labs(x = xlabel, y = ylabel) + \n  geom_point() + \n  geom_smooth()\nfig18_1\n#> `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\nwells$dist100 <- wells$dist/100\nfit_1 <- glm(switch ~ dist100, binomial(link = \"logit\"), data = wells)\nautoplot(fit_1)\nx <- predict(fit_1) #extract predicted values from the model\ny <- resid(fit_1) #extract the residuals from the model \nfig18_3 <- binnedplot(x, y)\ncoef(fit_1)\n#> (Intercept)     dist100 \n#>   0.6059594  -0.6218819\nconfint(fit_1)\n#> Waiting for profiling to be done...\n#>                  2.5 %     97.5 %\n#> (Intercept)  0.4882230  0.7246814\n#> dist100     -0.8140762 -0.4319795\nfig18_4 <- ggplot(wells, aes(x = dist, y = switch)) + \n    labs(x = xlabel, y = ylabel) + \n  geom_point() + \n  geom_smooth(method = \"glm\", method.args = \"binomial\")\nfig18_4\n#> `geom_smooth()` using formula 'y ~ x'\nfit_2 <- glm(switch ~ arsenic, binomial(link = \"logit\"), \n             data = wells)\ndisplay(fit_2)\n#> glm(formula = switch ~ arsenic, family = binomial(link = \"logit\"), \n#>     data = wells)\n#>             coef.est coef.se\n#> (Intercept) -0.31     0.07  \n#> arsenic      0.38     0.04  \n#> ---\n#>   n = 3020, k = 2\n#>   residual deviance = 4008.7, null deviance = 4118.1 (difference = 109.4)\nfig18_5 <- ggplot(wells, aes(x = arsenic, y = switch)) + \n    labs(x = \"Arsenic concentration\", y = ylabel) + \n  geom_point() + \n  geom_smooth(method = \"glm\", method.args = \"binomial\")\nfig18_5\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"glms-for-binary-data.html","id":"centering","chapter":"18 GLMs for binary data","heading":"18.3 Centering","text":"effect arsenic concentration distance nearest safe well can examined modelPrior fitting GLM, can make life easier centering explanatory variables subtracting mean value\napproach advantages regression intercept value zero unhelpful doesn’t make sense\napproach advantages regression intercept value zero unhelpful doesn’t make senseConsidering arsenic distance simultaneously introduces possibility interaction:display(fit_5) output:\nsecond row - effect change 100 m arsenic average level\nthird row - effect unit change arsenic well average distance\nfourth row (interaction) - every 100 m increase, additional -0.18 arsenic coefficient\nsecond row - effect change 100 m arsenic average levelthird row - effect unit change arsenic well average distancefourth row (interaction) - every 100 m increase, additional -0.18 arsenic coefficientCompare ratios estimates respective standard errors get sense effect coefficient\ninteraction term’s estimate even 2x big SE, interaction contaminant concentration distance going direction, probably isn’t significant\ncan look whether null hypotheses can rejected checking 95% CI:\ninteraction term’s estimate even 2x big SE, interaction contaminant concentration distance going direction, probably isn’t significantWe can look whether null hypotheses can rejected checking 95% CI:","code":"\nwells$c.dist100 <- wells$dist100 - mean(wells$dist100)\nwells$c.arsenic <- wells$arsenic - mean(wells$arsenic)\nhead(wells)\n#> # A tibble: 6 × 8\n#>   switch arsenic  dist assoc  educ dist100 c.dist100\n#>    <dbl>   <dbl> <dbl> <dbl> <dbl>   <dbl>     <dbl>\n#> 1      1    2.36  16.8     0     0   0.168   -0.315 \n#> 2      1    0.71  47.3     0     0   0.473   -0.0101\n#> 3      0    2.07  21.0     0    10   0.210   -0.274 \n#> 4      1    1.15  21.5     0    12   0.215   -0.268 \n#> 5      1    1.1   40.9     1    14   0.409   -0.0746\n#> 6      1    3.9   69.5     1     9   0.695    0.212 \n#> # … with 1 more variable: c.arsenic <dbl>\nfit_5 <- glm(switch ~ c.dist100 + c.arsenic + c.dist100:c.arsenic,\n       family = binomial, data = wells )\ndisplay(fit_5)\n#> glm(formula = switch ~ c.dist100 + c.arsenic + c.dist100:c.arsenic, \n#>     family = binomial, data = wells)\n#>                     coef.est coef.se\n#> (Intercept)          0.35     0.04  \n#> c.dist100           -0.87     0.10  \n#> c.arsenic            0.47     0.04  \n#> c.dist100:c.arsenic -0.18     0.10  \n#> ---\n#>   n = 3020, k = 4\n#>   residual deviance = 3927.6, null deviance = 4118.1 (difference = 190.5)\nconfint(fit_5)\n#> Waiting for profiling to be done...\n#>                          2.5 %      97.5 %\n#> (Intercept)          0.2732603  0.42951145\n#> c.dist100           -1.0808894 -0.66992679\n#> c.arsenic            0.3882846  0.55325092\n#> c.dist100:c.arsenic -0.3795821  0.02232171"},{"path":"conclusions.html","id":"conclusions","chapter":"19 Conclusions","heading":"19 Conclusions","text":"","code":""},{"path":"conclusions.html","id":"introduction-10","chapter":"19 Conclusions","heading":"19.1 Introduction","text":"text began analysis Space Shuttle Challenger distasterThe author concludes book re-analyzing data using keypoints covered past 17 chapters","code":"\ninstall.packages(\"arm\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"ggplot2\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"faraway\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"patchwork\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"dplyr\",  repos = \"https://cran.us.r-project.org\")\ninstall.packages(\"Sleuth3\",  repos = \"https://cran.us.r-project.org\")\nlibrary(arm)\nlibrary(ggplot2)\nlibrary(faraway)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(Sleuth3)"},{"path":"conclusions.html","id":"a-binomial-glm-analysis-of-the-challenger-binary-data","chapter":"19 Conclusions","heading":"19.2 A binomial GLM analysis of the Challenger binary data","text":"fuel leaks key dataset can expressed binary formThis dataset contains error, rectify :Plot Failure vs. launch temperature:Recharacterize binary form data:Create binomial GLM:Extract model’s coefficients:Examine 95% CI:Based negative slope 95% CI, clearly relationship failure temperature\nprobability fuel leak increases colder conditions\nprobability fuel leak increases colder conditionsPlot relationship:plot shows wide degree uncertainty around regression lineEven though lot uncertainty, uncertainty encompasses high degree system failThe temperature day catastrophic launch 30 degrees, well scope modelStill can extrapolate data (taking large grain salt) binomial GLMTaking sample size account can get t value two-tailed 95% CI n = 21:Use predict() function make predictions GLM:Create dataframe predicted ranges values around regression curveRedraw figure extrapolated probability values new range Temperature values:prediction even lowest level confidence unacceptably high probability failureCheck assumptions:values lie outside binned zones, binomial GLM good model data (even though binary)attempts made model data (Dalal et al. 1989), isn’t yet ideal solution dataStill, even residuals supporting model, poor decision even look data recommend launch day much lower launch temperature ever tested beforeThe author goes talk statistics just one tool must combined others\nexample, o-rings placed ice water become brittle \ndisciplines consulted \nexample, o-rings placed ice water become brittle tooOther disciplines consulted tooAn imperfect statistical analysis saved lives seven astronauts","code":"\nurlfile=\"https://raw.githubusercontent.com/apicellap/data/main/ex2011.csv\"\nex2011x<-read.csv(url(urlfile))\nstr(ex2011x)\n#> 'data.frame':    24 obs. of  2 variables:\n#>  $ Temperature: int  53 56 57 63 66 67 67 67 68 69 ...\n#>  $ Failure    : chr  \"Yes\" \"Yes\" \"Yes\" \"No\" ...\nhead(ex2011x)\n#>   Temperature Failure\n#> 1          53     Yes\n#> 2          56     Yes\n#> 3          57     Yes\n#> 4          63      No\n#> 5          66      No\n#> 6          67      No\nex2011x[4,2] <- \"Yes\" #It is always better document changes in a script and not alter the original dataframe\nfig19_1 <- ggplot(ex2011x, aes(x = Temperature, y = Failure)) + geom_point()\nfig19_1\nex2011x <- ex2011x %>% \n  mutate(failure = if_else(ex2011x$Failure == \"Yes\",1 , 0)) #if this statement is true then in column entitled 'failure' put 1, else put 0 \nhead(ex2011x)\n#>   Temperature Failure failure\n#> 1          53     Yes       1\n#> 2          56     Yes       1\n#> 3          57     Yes       1\n#> 4          63     Yes       1\n#> 5          66      No       0\n#> 6          67      No       0\nm1 <- glm(failure ~ Temperature, family = binomial(link = \"logit\"), data = ex2011x)\ncoef(m1)\n#> (Intercept) Temperature \n#>  14.1700665  -0.2155279\nconfint(m1)\n#> Waiting for profiling to be done...\n#>                  2.5 %      97.5 %\n#> (Intercept)  3.3162485 31.83327461\n#> Temperature -0.4724172 -0.05831438\nfig19_2 <- ggplot(ex2011x, aes(x = Temperature, y = failure)) + geom_point() + \n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"))\nfig19_2\n#> `geom_smooth()` using formula 'y ~ x'\nt21 <- qt(0.975, 21)\nt21\n#> [1] 2.079614\npredicts <- predict(m1, data.frame(Temperature = 30:85), \n                    se = TRUE)\nfit <- ilogit(predicts$fit) #ilogit() back transforms from logit to probability values \nupper <- ilogit(predicts$fit + t21 * predicts$se.fit)\nlower <-ilogit(predicts$fit - t21 * predicts$se.fit)\npredictions <- data.frame(fit, upper, lower, Temperature = 30:85)\nhead(predictions)\n#>         fit     upper     lower Temperature\n#> 1 0.9995493 0.9999999 0.4150330          30\n#> 2 0.9994409 0.9999998 0.4122349          31\n#> 3 0.9993066 0.9999997 0.4094173          32\n#> 4 0.9991399 0.9999995 0.4065783          33\n#> 5 0.9989333 0.9999992 0.4037160          34\n#> 6 0.9986771 0.9999988 0.4008283          35\nfig19_3 <- ggplot(ex2011x, aes(x = Temperature, y = failure)) + \n  xlim(30, 80) + \n  geom_point() + \n  geom_line(data = predictions, aes(Temperature, fit), colour = \"blue\") + \n  geom_line(data = predictions, aes(Temperature, upper), colour = \"darkgrey\") + \n  geom_line(data = predictions, aes(Temperature, lower), colour = \"darkgrey\") \nfig19_3\n#> Warning: Removed 1 rows containing missing values\n#> (geom_point).\n#> Warning: Removed 5 row(s) containing missing values (geom_path).\n#> Removed 5 row(s) containing missing values (geom_path).\n#> Removed 5 row(s) containing missing values (geom_path).\nx <- predict(m1)\ny <- resid(m1) \nfig19_4 <- binnedplot(x,y)"},{"path":"conclusions.html","id":"recommendations","chapter":"19 Conclusions","heading":"19.3 Recommendations","text":"Make research reproducibleThis can accomplished use Rmarkdown/quarto make research reportsA picture worth thousands wordsGraphs best way present dataGraph data analyzing understand itUse graphs assess assumptions/test diagnosticsKeep simpleTake time explore data look estimates/intervalsGet sense effect sizes uncertaintiesConsider one modelPresent 2-3 best models help showcase uncertain path towards model selectionAttempt P-free challengerFocus effect sizes estimatesReport estimates, intervals, sample sizesThe reproducibility crisis combatted reporting elements results section\nfacilitate meta-analysis\nfacilitate meta-analysisGo back basicsUnderstand basics error bars intervals plus interpret themMake focused plan analysisPlan data analyzed advanceMeet assumptionsGive P-values respect deserve ()assumptions met, end mushy p-valuesThe use p-values implemented assumptions met satisfaction conditions depend values meaningful possibleFocus repeatability, small p-valuesNo matter small p-value (high level confidence), result can still false positiveScientific results truly established shown repeatableAuthor says science values novelty much expense establishing solid foundation","code":""}]
